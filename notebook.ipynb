{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efab58a",
   "metadata": {},
   "source": [
    "## 3. WELCOME TO LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66717faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyoon/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_25744/292855851.py:4: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chat_response = chat.predict(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you mean “planets in our Solar System,” there are eight, in order from the Sun:  \n",
      "1. Mercury  \n",
      "2. Venus  \n",
      "3. Earth  \n",
      "4. Mars  \n",
      "5. Jupiter  \n",
      "6. Saturn  \n",
      "7. Uranus  \n",
      "8. Neptune  \n",
      "\n",
      "(Pluto and a handful of other bodies—Ceres, Eris, Haumea and Makemake—are classified as “dwarf planets” by the International Astronomical Union.)\n",
      "\n",
      "If you widen the scope to the rest of the galaxy (and beyond), astronomers have now confirmed over 5 000 exoplanets orbiting other stars—and ongoing surveys suggest there may be literally billions more. The total number of planets in the universe is unknown (and almost certainly vast).\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "question=\"How many planets are there?\"\n",
    "# chat_response = chat.predict(question) #this method(predict) will soon be deprecated\n",
    "chat_response = chat.invoke(question)\n",
    "print(chat_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87134bf4",
   "metadata": {},
   "source": [
    "### 3.1 Predict Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc135d05-1b92-4c36-af1c-a0405abfa31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='La distanza in linea d’aria tra Città del Messico e Parigi (capitali di Messico e Francia) è di circa 9.100 chilometri.  \\nMi chiamo Paolo.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 445, 'prompt_tokens': 52, 'total_tokens': 497, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-BOpSZ5ykJifXMVpgHmW83n3XXMp1S', 'finish_reason': 'stop', 'logprobs': None} id='run-0879f96f-2566-4069-a11b-f08a2aff606d-0' usage_metadata={'input_tokens': 52, 'output_tokens': 445, 'total_tokens': 497, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italin.\"),\n",
    "    AIMessage(content=\"Ciao! Mi chiamo Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and France? And what's your name?\")\n",
    "]\n",
    "\n",
    "# res = chat.predict_messages(messages) #this method(predict_messages) will be soon deprecated.\n",
    "res = chat.invoke(messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183fb4a",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Templates\n",
    "* template을 디스크에 저장하고 load할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16b96430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Η ευθύγραμμη (αεροπορική) απόσταση μεταξύ Πόλης του Μεξικού και Μπανγκόκ στην Ταϊλάνδη είναι περίπου 15\\xa0900 χιλιόμετρα.  \\nΤο όνομά μου είναι Βοηθός.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1040, 'prompt_tokens': 52, 'total_tokens': 1092, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 960, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-BOpeZkfHyYLZMyPpY2cKQTLEGLPBO', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f1c67e9-7445-4c06-8b18-226395c20f59-0', usage_metadata={'input_tokens': 52, 'output_tokens': 1040, 'total_tokens': 1092, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 960}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage # type: ignore\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "#PromptTempalte\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}?\")\n",
    "prompt = template.format(country_a=\"Korea\", country_b=\"France\")\n",
    "chat.invoke(prompt)\n",
    "\n",
    "#ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao! Mi chiamo {name}!\"),\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b}? And what's your name?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\"\n",
    ")\n",
    "\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a26639",
   "metadata": {},
   "source": [
    "### 3.3 OutputParser and LCEL\n",
    "* LCEL : LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a47ecd-7cc2-42ed-9ad6-dc2d160ce49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue',\n",
       " 'red',\n",
       " 'green',\n",
       " 'black',\n",
       " 'white',\n",
       " 'yellow',\n",
       " 'purple',\n",
       " 'orange',\n",
       " 'pink',\n",
       " 'brown']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import BaseOutputParser #type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma-seperated list of {max_items} in lowercase. DO NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question=\"Give me 10 most popular colors.\"\n",
    ")\n",
    "\n",
    "colors = chat.invoke(prompt)\n",
    "p.parse(colors.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7354a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charizard', 'eevee', 'mewtwo', 'bulbasaur']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain을 이용해서 위 코드를 개선할 수 있음\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import BaseOutputParser #type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma-seperated list of {max_items} in lowercase. DO NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\":5, \n",
    "    \"question\":\"Give me the most popular pokemons.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327eda2",
   "metadata": {},
   "source": [
    "### 3.4 Chaining Chains\n",
    "* callbacks & streaming=True : 답변 생성이 전부 완료될 때까지 기다리지 않음. 생성되는 즉시 결과를 확인.\n",
    "* callback: LLM에서 일어나는 다양한 event를 감지할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "085d1d02-7eba-4188-b3e5-aba338e85e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three classic, easy-to-find Korean dishes you can try at home. Pick one (or all three!), gather the ingredients, and follow the step‑by‑step.  \n",
      "\n",
      "1. Kimchi Fried Rice (Kimchi Bokkeumbap)  \n",
      "Ingredients (2 servings)  \n",
      " • 2 cups cold cooked rice (preferably a day old)  \n",
      " • 1 cup well-fermented kimchi, chopped (reserve 2 Tbsp kimchi juice)  \n",
      " • ½ onion, diced  \n",
      " • 1 small carrot, julienned (optional)  \n",
      " • 2 scallions, sliced  \n",
      " • 1 Tbsp gochujang (Korean red‑pepper paste) or 1 tsp gochugaru + 1 Tbsp soy sauce  \n",
      " • 1 Tbsp soy sauce  \n",
      " • 1 tsp sugar  \n",
      " • 1 tsp sesame oil  \n",
      " • 1 Tbsp neutral oil (vegetable/canola)  \n",
      " • 2 eggs  \n",
      " • Toasted sesame seeds (for garnish)  \n",
      "\n",
      "Steps  \n",
      "1. Prep: Heat a large nonstick skillet over medium–high.  \n",
      "2. Aromatics: Add neutral oil → sauté onion and carrot 2–3 min until soft.  \n",
      "3. Kimchi: Add chopped kimchi + 2 Tbsp kimchi juice → cook 2 min.  \n",
      "4. Seasoning: Stir in gochujang (or gochugaru+soy), soy sauce, sugar → mix.  \n",
      "5. Rice: Break up cold rice with your hands or a spoon, add to pan → stir‑fry, pressing out clumps, ~4 min.  \n",
      "6. Finish: Drizzle sesame oil, toss in half the scallions → taste & adjust (add more soy or gochujang).  \n",
      "7. Eggs: In a separate pan, fry eggs sunny‑side up (runny yolk recommended).  \n",
      "8. Serve: Divide rice into bowls, top each with an egg, remaining scallion & sesame seeds.\n",
      "\n",
      "Approx. time: 15 min  \n",
      "\n",
      "2. Beef Bulgogi (Korean‑Style Marinated Beef)  \n",
      "Ingredients (2–3 servings)  \n",
      " • 300 g (10 oz) thinly sliced beef sirloin or ribeye  \n",
      " • 3 Tbsp soy sauce  \n",
      " • 2 Tbsp brown sugar or honey  \n",
      " • 1 Tbsp mirin or dry white wine (optional)  \n",
      " • 2 cloves garlic, minced  \n",
      " • 1 tsp fresh ginger, grated  \n",
      " • 1 Tbsp sesame oil  \n",
      " • Pinch black pepper  \n",
      " • ½ small onion, thinly sliced  \n",
      " • 1 small carrot, thin matchsticks (optional)  \n",
      " • 3 scallions, cut into 2 cm pieces  \n",
      " • 1 tsp toasted sesame seeds  \n",
      "\n",
      "Steps  \n",
      "1. Marinade: In a bowl combine soy sauce, sugar, mirin, garlic, ginger, sesame oil and pepper. Add beef + onion + carrot; mix well. Marinate 20–30 min (or up to 2 hr in fridge).  \n",
      "2. Cook: Heat a skillet or grill‑pan over high heat. Add beef mixture in a single layer (do in batches to avoid steaming). Sear 1–2 min per side until just cooked.  \n",
      "3. Toss in scallions for 30 sec.  \n",
      "4. Serve: Plate with steamed rice, lettuce leaves for wraps, kimchi and ssamjang (optional).\n",
      "\n",
      "Approx. time: 45 min (incl. marinating)  \n",
      "\n",
      "3. Bibimbap (Mixed Rice Bowl)  \n",
      "Ingredients (2 servings)  \n",
      " • 2 cups cooked short‑grain rice  \n",
      " • 100 g (3½ oz) ground beef or thinly sliced beef (optional)  \n",
      " • ½ zucchini, julienned  \n",
      " • 1 carrot, julienned  \n",
      " • 1 cup spinach or bean sprouts  \n",
      " • 4–5 shiitake mushrooms, sliced  \n",
      " • 2 Tbsp soy sauce, divided  \n",
      " • 1 tsp sugar  \n",
      " • 1 clove garlic, minced (divided)  \n",
      " • 1 tsp sesame oil, divided  \n",
      " • Neutral oil for sautéing  \n",
      " • 2 eggs  \n",
      " • Gochujang to taste (start with 1 Tbsp per bowl)  \n",
      " • Sesame seeds, scallion slices  \n",
      "\n",
      "Steps  \n",
      "1. Prep vegetables in separate bowls:  \n",
      "   a. Spinach or sprouts: Blanch 1 min in boiling water (salted), drain, squeeze dry, toss with ½ tsp sesame oil, ½ tsp soy, ½ tsp minced garlic.  \n",
      "   b. Zucchini & carrot: Sauté each in 1 tsp oil over medium heat, season with a pinch of salt, set aside.  \n",
      "   c. Mushrooms: Sauté in 1 tsp oil + ½ tsp soy + a pinch sugar until soft.  \n",
      "   d. Beef (if using): Marinate 1 Tbsp soy + ½ tsp sugar + ½ tsp sesame oil + ½ tsp garlic, then sauté until just cooked.  \n",
      "2. Rice: Warm bowls of rice.  \n",
      "3. Assemble: Arrange each veggie/meat in sections on rice.  \n",
      "4. Egg: Fry eggs sunny‑side up, place on top.  \n",
      "5. Sauce & Serve: Dollop gochujang, sprinkle sesame seeds & scallions. To eat, mix everything thoroughly.\n",
      "\n",
      "Approx. time: 30 min  \n",
      "\n",
      "Tips for all dishes  \n",
      "• Adjust spice: add more/less gochujang or fresh chilies.  \n",
      "• Garnish: fresh cucumber slices or a side of store‑bought kimchi.  \n",
      "• Equipment: a good nonstick pan or well‑seasoned cast iron makes stir‑fries easier.  \n",
      "\n",
      "Enjoy your Korean feast! Let me know if you’d like more details or another recipe (seafood pancakes, tteokbokki, etc.).Here’s how you can turn each of these into a vegetarian (or even vegan) dish, with minimal tweaks to the original recipes.  \n",
      "\n",
      "1. Kimchi Fried Rice  \n",
      "   • Kimchi: Most store‑bought kimchis use fish sauce or shrimp paste. Look for “vegan kimchi” at Korean markets or make your own by substituting the fish ingredients with a little crushed nori (seaweed) and a splash of soy sauce or tamari in your chili paste.  \n",
      "   • Eggs (optional): If you eat eggs, keep the sunny‑side‑up as written. For a vegan version, crumble firm tofu and sauté it in a little oil, a pinch of turmeric for color, plus salt and black salt (kala namak) for that eggy aroma. Stir it in at the very end or serve on top.  \n",
      "\n",
      "2. “Mushroom Bulgogi” (in place of Beef Bulgogi)  \n",
      "   Ingredients (2–3 servings)  \n",
      "    • 300 g (10 oz) king oyster mushrooms (or shiitakes + cremini), thinly sliced lengthwise into “scallops”  \n",
      "    • 3 Tbsp soy sauce  \n",
      "    • 2 Tbsp brown sugar or maple syrup  \n",
      "    • 1 Tbsp mirin or dry white wine (optional)  \n",
      "    • 2 cloves garlic, minced  \n",
      "    • 1 tsp fresh ginger, grated  \n",
      "    • 1 Tbsp sesame oil  \n",
      "    • Pinch black pepper  \n",
      "    • ½ small onion, thinly sliced  \n",
      "    • 3 scallions, cut into 2 cm pieces  \n",
      "    • 1 tsp toasted sesame seeds  \n",
      "\n",
      "   Steps  \n",
      "    1. Marinade: Whisk together soy sauce, sugar (or syrup), mirin, garlic, ginger, sesame oil and pepper. Toss in the mushroom slices and onion. Marinate 15–20 minutes.  \n",
      "    2. Cook: Heat a skillet or grill pan over high heat. Add the mushrooms in a single layer, sear 2 minutes per side—do batches if needed.  \n",
      "    3. In the last 30 sec, stir in scallions, then sprinkle with sesame seeds.  \n",
      "    4. Serve with rice, lettuce leaves and gochujang/ssamjang on the side.  \n",
      "\n",
      "3. Vegetarian Bibimbap  \n",
      "   • Omit the ground beef. Instead:  \n",
      "     – Sauté 150 g crumbled firm tofu (or tempeh) in 1 Tbsp soy sauce, ½ tsp sugar, ½ tsp sesame oil and ½ tsp garlic until lightly browned.  \n",
      "     – Or use the same mushroom mix from above: thinly sliced king oysters, marinated briefly in 1 Tbsp soy + ½ tsp sugar + ½ tsp sesame oil, then sauté.  \n",
      "   • Eggs: Keep them sunny‑side‑up if you eat eggs. For vegan, sprinkle fried tofu cubes or pan‑crispy chickpeas on top instead.  \n",
      "   • All other veg prep stays the same—zucchini, carrot, spinach/beansprouts, mushrooms.  \n",
      "\n",
      "General tips  \n",
      " • Sesame oil, soy and garlic do the heavy lifting on flavor—your swaps just need to soak those in.  \n",
      " • If you need extra umami, add a little mushroom stock powder or a dash of vegan Worcestershire.  \n",
      " • Garnish with plenty of toasted sesame seeds and scallions, and serve with gochujang on the side so everyone can adjust the heat.  \n",
      "\n",
      "Enjoy your meat‑free Korean feast!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here’s how you can turn each of these into a vegetarian (or even vegan) dish, with minimal tweaks to the original recipes.  \\n\\n1. Kimchi Fried Rice  \\n   • Kimchi: Most store‑bought kimchis use fish sauce or shrimp paste. Look for “vegan kimchi” at Korean markets or make your own by substituting the fish ingredients with a little crushed nori (seaweed) and a splash of soy sauce or tamari in your chili paste.  \\n   • Eggs (optional): If you eat eggs, keep the sunny‑side‑up as written. For a vegan version, crumble firm tofu and sauté it in a little oil, a pinch of turmeric for color, plus salt and black salt (kala namak) for that eggy aroma. Stir it in at the very end or serve on top.  \\n\\n2. “Mushroom Bulgogi” (in place of Beef Bulgogi)  \\n   Ingredients (2–3 servings)  \\n    • 300\\xa0g (10\\xa0oz) king oyster mushrooms (or shiitakes + cremini), thinly sliced lengthwise into “scallops”  \\n    • 3\\xa0Tbsp soy sauce  \\n    • 2\\xa0Tbsp brown sugar or maple syrup  \\n    • 1\\xa0Tbsp mirin or dry white wine (optional)  \\n    • 2 cloves garlic, minced  \\n    • 1\\xa0tsp fresh ginger, grated  \\n    • 1\\xa0Tbsp sesame oil  \\n    • Pinch black pepper  \\n    • ½ small onion, thinly sliced  \\n    • 3 scallions, cut into 2\\xa0cm pieces  \\n    • 1\\xa0tsp toasted sesame seeds  \\n\\n   Steps  \\n    1. Marinade: Whisk together soy sauce, sugar (or syrup), mirin, garlic, ginger, sesame oil and pepper. Toss in the mushroom slices and onion. Marinate 15–20\\xa0minutes.  \\n    2. Cook: Heat a skillet or grill pan over high heat. Add the mushrooms in a single layer, sear 2\\xa0minutes per side—do batches if needed.  \\n    3. In the last 30\\xa0sec, stir in scallions, then sprinkle with sesame seeds.  \\n    4. Serve with rice, lettuce leaves and gochujang/ssamjang on the side.  \\n\\n3. Vegetarian Bibimbap  \\n   • Omit the ground beef. Instead:  \\n     – Sauté 150\\xa0g crumbled firm tofu (or tempeh) in 1\\xa0Tbsp soy sauce, ½\\xa0tsp sugar, ½\\xa0tsp sesame oil and ½\\xa0tsp garlic until lightly browned.  \\n     – Or use the same mushroom mix from above: thinly sliced king oysters, marinated briefly in 1\\xa0Tbsp soy + ½\\xa0tsp sugar + ½\\xa0tsp sesame oil, then sauté.  \\n   • Eggs: Keep them sunny‑side‑up if you eat eggs. For vegan, sprinkle fried tofu cubes or pan‑crispy chickpeas on top instead.  \\n   • All other veg prep stays the same—zucchini, carrot, spinach/beansprouts, mushrooms.  \\n\\nGeneral tips  \\n • Sesame oil, soy and garlic do the heavy lifting on flavor—your swaps just need to soak those in.  \\n • If you need extra umami, add a little mushroom stock powder or a dash of vegan Worcestershire.  \\n • Garnish with plenty of toasted sesame seeds and scallions, and serve with gochujang on the side so everyone can adjust the heat.  \\n\\nEnjoy your meat‑free Korean feast!', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'o4-mini-2025-04-16'}, id='run-2515a9da-4ae9-48e8-bd72-9ae9f6078bac-0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "#normal recipes\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class international chef. You create easy-to-follow recipes for any type of cuising with easy-to-find ingredients.\"),\n",
    "    (\"human\", \"I want to cook {country} food.\")\n",
    "])\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "#vegetarian recipes\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized in making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative, just say you don't know how to replace it.\"),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "veg_chef_chain = veg_chef_prompt | chat\n",
    "\n",
    "#mixing two chains\n",
    "final_chain = {\"recipe\":chef_chain} | veg_chef_chain\n",
    "\n",
    "#calling OpenAI API\n",
    "final_chain.invoke({\n",
    "    \"country\":\"korean\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9cda6",
   "metadata": {},
   "source": [
    "## 4. MODEL IO\n",
    "* <b>Modules</b>: Model I/O, Retrieval, Chains, Agents, Memory, Callbacks\n",
    "* <b>Model I/O</b> : Interface with language models<br/>\n",
    "   - prompts, language models, output parsers\n",
    "* <b>Retrieval</b>: 외부 데이터로 접근하여 이를 모델에 제공\n",
    "   - document loaders, transformers, text embedding, vector stores, retrievers\n",
    "* <b>Agents</b>: AI가 독립적으로 작동할 수 있게 함.\n",
    "* <b>Callbacks</b>: Log and stream intermediate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5000c-0b23-4a09-87a1-e0c0cb53fb0e",
   "metadata": {},
   "source": [
    "### 4.1 FewShotPromptTemplate\n",
    "* 모델에게 예제를 주는 것과 같음 (대답 포맷 등). 답변의 형식을 구구절절 설명하는 것보다 더 빠르고 정확할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b862f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PromptTemplate을 이렇게 만들 수도 있음 (그냥 알고 있으라고)\n",
    "t = PromptTemplate(\n",
    "    template = \"What is the capital of {country}?\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "#same as PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d548783a-6048-4a57-98ba-c85e9a0b07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The capital is Warsaw, the language is Polish, and the food includes pierogi, kielbasa, and bigos."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: The capital is Warsaw, the language is Polish, and the food includes pierogi, kielbasa, and bigos.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-400ce6e5-c9b3-44c7-97c8-f8ae7dbe8190-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"question\":\"What do you know about France?\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"question\":\"What do you know about Korea?\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"question\":\"What do you know about Italy?\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"question\":\"What do you know about Japan?\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\":\"Poland\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12904b-ca98-44bf-ae83-3f6fba046d3e",
   "metadata": {},
   "source": [
    "### 4.2 FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "666455a4-2da9-4b98-acf4-a3b0daf10e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital is Washington D.C., \n",
      "language is English, \n",
      "food is burgers and apple pie."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='capital is Washington D.C., \\nlanguage is English, \\nfood is burgers and apple pie.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-343e758b-b342-4748-9a4a-df05eab6528d-0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate # type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"country\":\"France\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"country\":\"Korea\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"country\":\"Italy\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"country\":\"Japan\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\",\"{answer}\")\n",
    "])\n",
    "\n",
    "fewshot_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a geography expert\"),\n",
    "    fewshot_example_prompt, #이전의 모든 example들\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke(\"USA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c8971-6202-4892-bdca-671cfea9d662",
   "metadata": {},
   "source": [
    "### 4.3 LengthBasedExampleSelector\n",
    "* dynamic하게 example들을 선택 -> 셀 수 없을 정도로 많은 example들이 있을 수 있음. 전부 모델한테 줄 수는 없어.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e901f2a8-4d44-4ae7-bd56-c5dcd3210c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:What do you know about France?\\nAI:capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\\n\\nHuman: What do you know about Netherlands?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"question\":\"What do you know about France?\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"question\":\"What do you know about Korea?\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"question\":\"What do you know about Italy?\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"question\":\"What do you know about Japan?\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    max_length=30\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    example_selector = example_selector, #전에는 그냥 모든 example들을 넣었음\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d591a5-057e-403a-80f4-fda904130e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:What do you know about Italy?\\nAI:capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\\n\\nHuman: What do you know about Netherlands?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤하게 example들을 선택\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"question\":\"What do you know about France?\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"question\":\"What do you know about Korea?\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"question\":\"What do you know about Italy?\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"question\":\"What do you know about Japan?\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "# 나만의 example selector를 만들 수 있다는 간단한 예시\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example): #ExampleSelector 내부에 반드시 있어야 함 (없으면 에러)\n",
    "        return super().add_example(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)] #하나의 example만 선택해서 리턴\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    example_selector = example_selector, #전에는 그냥 모든 example들을 넣었음\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb2f21-4937-4a45-8e82-978e71b5eeeb",
   "metadata": {},
   "source": [
    "### 4.4 Serialization and Composition\n",
    "* serialize(직렬화): 불러오기, 저장 등\n",
    "* compose: 작은 prompt templates들을 결합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eeec081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of China?\n",
      "What is the capital of China?\n"
     ]
    }
   ],
   "source": [
    "# prompt 불러오기\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "print(prompt.format(country=\"China\"))\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "print(prompt.format(country=\"China\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f77405d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_25744/1575831411.py:52: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  full_prompt = PipelinePromptTemplate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr matey! Me favorite food be grilled fish and salted pork, aye! Nothing like a good meal to keep me strength up on the high seas. Arrrr!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrr matey! Me favorite food be grilled fish and salted pork, aye! Nothing like a good meal to keep me strength up on the high seas. Arrrr!', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-8c1d7fe1-eb46-44e4-911a-06350539baab-0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 개의 prompt를 하나로 합치기\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}.\n",
    "\"\"\"\n",
    ")\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human:{example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "start=PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts=[\n",
    "    (\"intro\",intro),\n",
    "    (\"example\",example),\n",
    "    (\"start\",start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\":\"Pirate\",\n",
    "        \"example_question\":\"What is your location?\",\n",
    "        \"example_answer\":\"Arrrrg! That is a scret!! Arg arrrg!\",\n",
    "        \"question\": \"What is your favorite food?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56f1d0",
   "metadata": {},
   "source": [
    "### 4.5 Caching\n",
    "* 질문, 답변을 저장. 중복되는 API 콜을 줄일 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d374944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ingredients:\\n- 2 cups cooked rice\\n- 1 cup kimchi, chopped\\n- 1/2 cup kimchi juice\\n- 1/2 cup diced pork belly or bacon\\n- 1/2 onion, diced\\n- 2 cloves garlic, minced\\n- 1 tbsp gochujang (Korean red chili paste)\\n- 1 tbsp soy sauce\\n- 1 tbsp sesame oil\\n- 1 green onion, chopped\\n- 1 fried egg (optional)\\n- Sesame seeds for garnish\\n\\nInstructions:\\n1. Heat a large skillet over medium heat and add the diced pork belly or bacon. Cook until browned and crispy.\\n2. Add the diced onion and minced garlic to the skillet and cook until softened.\\n3. Add the chopped kimchi and cook for a few minutes until heated through.\\n4. Stir in the gochujang, soy sauce, and kimchi juice. Mix well.\\n5. Add the cooked rice to the skillet and stir to combine with the kimchi mixture.\\n6. Drizzle with sesame oil and continue to cook for a few more minutes, stirring occasionally.\\n7. Remove from heat and garnish with chopped green onion and sesame seeds.\\n8. Serve hot with a fried egg on top, if desired. Enjoy your delicious kimchi fried rice!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 270, 'prompt_tokens': 21, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BP6MpaJDcZEoFojgHlPwgOgCxGorB', 'finish_reason': 'stop', 'logprobs': None}, id='run-540e0871-6706-4956-8ea1-62e82cec7170-0', usage_metadata={'input_tokens': 21, 'output_tokens': 270, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "\n",
    "# set_llm_cache(InMemoryCache()) #이제부터 모든 response가 메모리에 저장됨. 컴퓨터를 재시동하면 없어짐.\n",
    "set_llm_cache(SQLiteCache(\"cache.db\")) #sqlite 데이터베이스 생성. 여기에 response를 저장함.\n",
    "set_debug(True) #로그를 보여줌. 나중에 체인 작업을 할 때 매우 유용\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "chat.invoke(\"Give me a recipe of 김치볶음밥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50fccc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ingredients:\\n- 2 cups cooked rice\\n- 1 cup kimchi, chopped\\n- 1/2 cup kimchi juice\\n- 1/2 cup diced pork belly or bacon\\n- 1/2 onion, diced\\n- 2 cloves garlic, minced\\n- 1 tbsp gochujang (Korean red chili paste)\\n- 1 tbsp soy sauce\\n- 1 tbsp sesame oil\\n- 1 green onion, chopped\\n- 1 fried egg (optional)\\n- Sesame seeds for garnish\\n\\nInstructions:\\n1. Heat a large skillet over medium heat and add the diced pork belly or bacon. Cook until browned and crispy.\\n2. Add the diced onion and minced garlic to the skillet and cook until softened.\\n3. Add the chopped kimchi and cook for a few minutes until heated through.\\n4. Stir in the gochujang, soy sauce, and kimchi juice. Mix well.\\n5. Add the cooked rice to the skillet and stir to combine with the kimchi mixture.\\n6. Drizzle with sesame oil and continue to cook for a few more minutes, stirring occasionally.\\n7. Remove from heat and garnish with chopped green onion and sesame seeds.\\n8. Serve hot with a fried egg on top, if desired. Enjoy your delicious kimchi fried rice!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 270, 'prompt_tokens': 21, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BP6MpaJDcZEoFojgHlPwgOgCxGorB', 'finish_reason': 'stop', 'logprobs': None}, id='run-540e0871-6706-4956-8ea1-62e82cec7170-0', usage_metadata={'input_tokens': 21, 'output_tokens': 270, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"Give me a recipe of 김치볶음밥\") #똑같은 질문을 두 번째 함 -> 답변하는데 0초 소요. 메모리에서 들고 왔으니까."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd09d87",
   "metadata": {},
   "source": [
    "### 4.6 Serialization\n",
    "* 모델을 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d7bdc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 430\n",
      "\tPrompt Tokens: 22\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 408\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0006230000000000001\n",
      "content='Kimchi is a traditional Korean side dish made of fermented vegetables, most commonly napa cabbage and Korean radish. Here is a simple recipe to make kimchi at home:\\n\\nIngredients:\\n- 1 napa cabbage\\n- 1 Korean radish\\n- 1/2 cup of coarse sea salt\\n- 1/4 cup of Korean red pepper flakes (gochugaru)\\n- 1 tablespoon of minced garlic\\n- 1 tablespoon of minced ginger\\n- 1 tablespoon of fish sauce\\n- 1 tablespoon of sugar\\n- 2 green onions, chopped\\n- 1 carrot, julienned\\n\\nInstructions:\\n1. Cut the napa cabbage in half lengthwise, then cut each half into quarters. Remove the core and chop the cabbage into bite-sized pieces. Peel the Korean radish and cut it into thin matchsticks.\\n\\n2. In a large bowl, dissolve the sea salt in water. Add the cabbage and radish, making sure they are fully submerged in the brine. Let it sit for 2-3 hours, tossing occasionally.\\n\\n3. Rinse the cabbage and radish under cold water to remove excess salt. Drain well and set aside.\\n\\n4. In a separate bowl, mix together the red pepper flakes, garlic, ginger, fish sauce, and sugar to make a paste.\\n\\n5. Add the green onions and carrot to the paste, then mix in the cabbage and radish until well combined.\\n\\n6. Pack the kimchi into a clean glass jar, pressing down to remove any air bubbles. Leave some space at the top of the jar as the kimchi will expand during fermentation.\\n\\n7. Cover the jar loosely with a lid and let it sit at room temperature for 1-2 days to ferment. Taste the kimchi daily until it reaches your desired level of fermentation.\\n\\n8. Once the kimchi is ready, store it in the refrigerator to slow down the fermentation process. Enjoy your homemade kimchi as a side dish or in various Korean dishes!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 408, 'prompt_tokens': 22, 'total_tokens': 430, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPC2wfwLPNVikfQm1t1PxBictdWaF', 'finish_reason': 'stop', 'logprobs': None} id='run-9212a912-151a-4e3c-830d-ca238fa739f1-0' usage_metadata={'input_tokens': 22, 'output_tokens': 408, 'total_tokens': 430, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 한 번 API 콜을 할 떄 소요되는 비용 체크\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.callbacks import get_openai_callback # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    kimchi_recipe = chat.invoke(\"Tell me how I can make Kimchi(겉절이)\")\n",
    "    print(usage)\n",
    "    print(usage.total_cost) #가격에 대해서만 알고 싶다면\n",
    "    print(usage.prompt_tokens) #내가 모델에 건네준 토큰\n",
    "    print(usage.completion_tokens) #모델이 생성한 토큰\n",
    "\n",
    "print(kimchi_recipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0dd993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장하기\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_completion_tokens=500, #말 많은거 싫어\n",
    "    temperature=0.1 #너무 창의적인거 싫어\n",
    ")\n",
    "# chat.save(\"model.json\") #model.json 파일이 생성됨 -> 더 이상 지원하지 않음. joblib 혹은 pickle을 사용해야 함.\n",
    "\n",
    "# #모델 불러오기\n",
    "from langchain.llms.loading import load_llm\n",
    "# chat = load_llm(\"model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504338e9",
   "metadata": {},
   "source": [
    "## 5. MEMORY\n",
    "* OpenAI의 Chat은 기본적으로 memory를 제공하지 않음 (stateless)\n",
    "* 랭체인을 이용해서 implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c695340",
   "metadata": {},
   "source": [
    "### 5.0 ConversationBufferMemory\n",
    "* <b>Conversation Buffer Memory</b>: 대화 전체를 저장. prompt가 길어짐 -> 비싸!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4a233ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)#return_messages=False를 주면 메시지 객체가 아니라 문자열을 반환\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285bce5",
   "metadata": {},
   "source": [
    "### 5.1 ConversationBufferWindowMemory\n",
    "* <b>Conversation Buffer Window Memory</b>: 최근 n개의 대화를 저장 -> 최근 대화만 기억할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b9bbf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='5', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='5', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(\"1\",\"1\") #이 부분은 버려짐\n",
    "add_message(\"2\",\"2\")\n",
    "add_message(\"3\",\"3\")\n",
    "add_message(\"4\",\"4\")\n",
    "add_message(\"5\",\"5\")\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aef977",
   "metadata": {},
   "source": [
    "### 5.2 ConversationSummaryMemory\n",
    "* <b>Conversation Summary Memory</b>: llm을 이용. 대화가 길어질수록 유용해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0ed9388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Boyon from Paris. The AI responds with enthusiasm at the information shared, expressing a desire to visit the beautiful city of Paris.'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi! I'm Boyon, I live in Paris.\", \"Wow that is so cool!\")\n",
    "add_message(\"Paris is very beautiful.\", \"I wish I could go\")\n",
    "get_history() #진행된 대화가 별로 없음 -> 초반엔 대화보다 summary가 더 긺."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec362e",
   "metadata": {},
   "source": [
    "### 5.3 ConversationSummaryBufferMemory\n",
    "* <b>Conversation Summary Buffer Memory</b>: max_token_limit에 달하면, 최근 메시지를 제외한 메시지들을 summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65c1ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_25744/1068426509.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='Boyon introduces themselves and mentions they live in Paris. The AI expresses admiration for this fact. Boyon agrees, stating that Paris is very beautiful.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=20, #최근 max_token_limit개 토큰의 interactions를 제외하고 summarize\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi! I'm Boyon, I live in Paris.\", \"Wow that is so cool!\")\n",
    "add_message(\"Paris is very beautiful.\", \"I wish I could go\")\n",
    "get_history() #진행된 대화가 별로 없음 -> 초반엔 대화보다 summary가 더 긺."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fe54f",
   "metadata": {},
   "source": [
    "### 5.4 ConversationKGMemory\n",
    "* <b>Conversational Knowledge Graph Memory</b>: 대화 중인 entity의 knowledge graph를 만듦.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf069c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [SystemMessage(content='On Boyon: Boyon lives in Paris.', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory=ConversationKGMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "\n",
    "add_message(\"Hi! I'm Boyon, I live in Paris.\", \"Wow that is so cool!\")\n",
    "# print(memory.load_memory_variables({\"input\":\"Who is Boyoon?\"})) #entity의 knowledge graph를 만들어 요약본을 생성, 메모리에 저장\n",
    "add_message(\"I love Kimchi.\", \"That's cool.\")\n",
    "print(memory.load_memory_variables({\"input\":\"Who is Boyoon?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d578183",
   "metadata": {},
   "source": [
    "### 5.5 Memory on LLMChain\n",
    "* LLM chain: off-the-shelf chain. 일반적인 목적. 빠르게 시작할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04bc1cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    \n",
      "    Human: My name is Boyoon.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Nice to meet you, Boyoon! How can I assist you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Human: My name is Boyoon.\n",
      "AI: Nice to meet you, Boyoon! How can I assist you today?\n",
      "    Human: I live in Paris since 7 years ago, but I was born and raised in Seoul.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's interesting! How do you like living in Paris compared to Seoul?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    System: The human introduces themselves as Boyoon. The AI responds by saying \"Nice to meet you, Boyoon! How can I assist you today?\"\n",
      "Human: I live in Paris since 7 years ago, but I was born and raised in Seoul.\n",
      "AI: That's interesting! How do you like living in Paris compared to Seoul?\n",
      "    Human: While I was traveling, a stranger asked me where I'm from. How do I answer to that?\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You can simply answer by saying \"I'm originally from Seoul, but I currently live in Paris.\"\n"
     ]
    }
   ],
   "source": [
    "# LLMChain에 우리가 가진 메모리 넣기\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key = \"chat_history\" #template안에 넣을 history의 위치를 알려줌\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True, #chain의 프롬프트 로그들을 확인할 수 있음\n",
    ")\n",
    "\n",
    "print(chain.predict(question=\"My name is Boyoon.\"))\n",
    "print(chain.predict(question=\"I live in Paris since 7 years ago, but I was born and raised in Seoul.\"))\n",
    "print(chain.predict(question=\"While I was traveling, a stranger asked me where I'm from. How do I answer to that?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40038a5e",
   "metadata": {},
   "source": [
    "### 5.6 Chat Based Memory\n",
    "* memory.load_memory_variables({})가 문자열이 아니라 메시지 객체를 반환하게 합시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263350cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain에 우리가 가진 메모리 넣기\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), #누가 보냈는지 알 수 없고 예측하기 어려운 양의 메시지를 넣을 수 있음\n",
    "    (\"human\":{question}),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(chain.predict(question=\"My name is Boyoon.\"))\n",
    "print(chain.predict(question=\"I live in Paris since 7 years ago, but I was born and raised in Seoul.\"))\n",
    "print(chain.predict(question=\"What's my name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620db9a",
   "metadata": {},
   "source": [
    "### 5.7 LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ba640dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'question'] input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10ad8a430>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI talking to a human.'), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]\n",
      "{'question': 'My name is Boyoon'}\n",
      "content='Nice to meet you, Boyoon! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 26, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPw8AVifvIgVO1ZJOPgfKSzpRvVKp', 'finish_reason': 'stop', 'logprobs': None} id='run-d289ca97-a5de-48f3-a674-c7e6bd54b8f0-0' usage_metadata={'input_tokens': 26, 'output_tokens': 16, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "{'question': 'What is my name?'}\n",
      "content='Your name is Boyoon.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 54, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPw8CIWUgLLo5ouoTXbu6FsOpkMTx', 'finish_reason': 'stop', 'logprobs': None} id='run-8df1fb25-ad53-437d-ab88-789fc68c4098-0' usage_metadata={'input_tokens': 54, 'output_tokens': 7, 'total_tokens': 61, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key = \"history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), #누가 보냈는지 알 수 없고 예측하기 어려운 양의 메시지를 넣을 수 있음\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "    print(_)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm \n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\":question})\n",
    "    memory.save_context(\n",
    "        {\"input\":question}, \n",
    "        {\"output\":result.content}\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"My name is Boyoon\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43116876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5082c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
