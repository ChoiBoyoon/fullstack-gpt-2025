{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efab58a",
   "metadata": {},
   "source": [
    "## 3. WELCOME TO LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66717faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='If you mean “planets in our Solar System,” there are eight currently recognized planets (in order from the Sun):  \\n 1. Mercury  \\n 2. Venus  \\n 3. Earth  \\n 4. Mars  \\n 5. Jupiter  \\n 6. Saturn  \\n 7. Uranus  \\n 8. Neptune  \\n\\nPluto was reclassified as a “dwarf planet” by the International Astronomical Union in 2006, so it’s no longer counted among the eight.\\n\\nBeyond our Solar System, astronomers have confirmed over 5,500 exoplanets orbiting other stars (as of mid-2024), and thousands more candidates await confirmation. Estimates suggest our Milky Way alone may host on the order of 100–400 billion planets, and when you multiply that by the roughly two trillion galaxies in the observable universe, the total number of planets likely runs into the 10^23–10^24 range.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 533, 'prompt_tokens': 12, 'total_tokens': 545, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-BqeVNI8BuHB28kJjqdymoqSHzbm0t', 'finish_reason': 'stop', 'logprobs': None} id='run-ce771561-0312-44a1-ad84-3886d5586869-0' usage_metadata={'input_tokens': 12, 'output_tokens': 533, 'total_tokens': 545, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "question=\"How many planets are there?\"\n",
    "# chat_response = chat.predict(question) #this method(predict) will soon be deprecated\n",
    "chat_response = chat.invoke(question)\n",
    "print(chat_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87134bf4",
   "metadata": {},
   "source": [
    "### 3.1 Predict Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc135d05-1b92-4c36-af1c-a0405abfa31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='La distanza in linea d’aria tra Città del Messico e Parigi (capitali di Messico e Francia) è di circa 9.100 chilometri.  \\nMi chiamo Paolo.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 445, 'prompt_tokens': 52, 'total_tokens': 497, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-BOpSZ5ykJifXMVpgHmW83n3XXMp1S', 'finish_reason': 'stop', 'logprobs': None} id='run-0879f96f-2566-4069-a11b-f08a2aff606d-0' usage_metadata={'input_tokens': 52, 'output_tokens': 445, 'total_tokens': 497, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a geography expert. And you only reply in Italin.\"),\n",
    "    AIMessage(content=\"Ciao! Mi chiamo Paolo!\"),\n",
    "    HumanMessage(content=\"What is the distance between Mexico and France? And what's your name?\")\n",
    "]\n",
    "\n",
    "# res = chat.predict_messages(messages) #this method(predict_messages) will be soon deprecated.\n",
    "res = chat.invoke(messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183fb4a",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Templates\n",
    "* template을 디스크에 저장하고 load할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16b96430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Η ευθύγραμμη (αεροπορική) απόσταση μεταξύ Πόλης του Μεξικού και Μπανγκόκ στην Ταϊλάνδη είναι περίπου 15\\xa0900 χιλιόμετρα.  \\nΤο όνομά μου είναι Βοηθός.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1040, 'prompt_tokens': 52, 'total_tokens': 1092, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 960, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-BOpeZkfHyYLZMyPpY2cKQTLEGLPBO', 'finish_reason': 'stop', 'logprobs': None}, id='run-8f1c67e9-7445-4c06-8b18-226395c20f59-0', usage_metadata={'input_tokens': 52, 'output_tokens': 1040, 'total_tokens': 1092, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 960}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage # type: ignore\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "#PromptTempalte\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}?\")\n",
    "prompt = template.format(country_a=\"Korea\", country_b=\"France\")\n",
    "chat.invoke(prompt)\n",
    "\n",
    "#ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "    (\"ai\", \"Ciao! Mi chiamo {name}!\"),\n",
    "    (\"human\", \"What is the distance between {country_a} and {country_b}? And what's your name?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language=\"Greek\",\n",
    "    name=\"Socrates\",\n",
    "    country_a=\"Mexico\",\n",
    "    country_b=\"Thailand\"\n",
    ")\n",
    "\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a26639",
   "metadata": {},
   "source": [
    "### 3.3 OutputParser and LCEL\n",
    "* LCEL : LangChain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a47ecd-7cc2-42ed-9ad6-dc2d160ce49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue',\n",
       " 'red',\n",
       " 'green',\n",
       " 'black',\n",
       " 'white',\n",
       " 'yellow',\n",
       " 'purple',\n",
       " 'orange',\n",
       " 'pink',\n",
       " 'brown']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import BaseOutputParser #type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma-seperated list of {max_items} in lowercase. DO NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    max_items = 10,\n",
    "    question=\"Give me 10 most popular colors.\"\n",
    ")\n",
    "\n",
    "colors = chat.invoke(prompt)\n",
    "p.parse(colors.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7354a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pikachu', 'charizard', 'eevee', 'mewtwo', 'bulbasaur']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain을 이용해서 위 코드를 개선할 수 있음\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.schema import BaseOutputParser #type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"o4-mini\")\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    def parse(self, text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a list generating machine. Everything you are asked will be answered with a comma-seperated list of {max_items} in lowercase. DO NOT reply with anything else.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"max_items\":5, \n",
    "    \"question\":\"Give me the most popular pokemons.\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327eda2",
   "metadata": {},
   "source": [
    "### 3.4 Chaining Chains\n",
    "* callbacks & streaming=True : 답변 생성이 전부 완료될 때까지 기다리지 않음. 생성되는 즉시 결과를 확인.\n",
    "* callback: LLM에서 일어나는 다양한 event를 감지할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "085d1d02-7eba-4188-b3e5-aba338e85e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three classic, easy-to-find Korean dishes you can try at home. Pick one (or all three!), gather the ingredients, and follow the step‑by‑step.  \n",
      "\n",
      "1. Kimchi Fried Rice (Kimchi Bokkeumbap)  \n",
      "Ingredients (2 servings)  \n",
      " • 2 cups cold cooked rice (preferably a day old)  \n",
      " • 1 cup well-fermented kimchi, chopped (reserve 2 Tbsp kimchi juice)  \n",
      " • ½ onion, diced  \n",
      " • 1 small carrot, julienned (optional)  \n",
      " • 2 scallions, sliced  \n",
      " • 1 Tbsp gochujang (Korean red‑pepper paste) or 1 tsp gochugaru + 1 Tbsp soy sauce  \n",
      " • 1 Tbsp soy sauce  \n",
      " • 1 tsp sugar  \n",
      " • 1 tsp sesame oil  \n",
      " • 1 Tbsp neutral oil (vegetable/canola)  \n",
      " • 2 eggs  \n",
      " • Toasted sesame seeds (for garnish)  \n",
      "\n",
      "Steps  \n",
      "1. Prep: Heat a large nonstick skillet over medium–high.  \n",
      "2. Aromatics: Add neutral oil → sauté onion and carrot 2–3 min until soft.  \n",
      "3. Kimchi: Add chopped kimchi + 2 Tbsp kimchi juice → cook 2 min.  \n",
      "4. Seasoning: Stir in gochujang (or gochugaru+soy), soy sauce, sugar → mix.  \n",
      "5. Rice: Break up cold rice with your hands or a spoon, add to pan → stir‑fry, pressing out clumps, ~4 min.  \n",
      "6. Finish: Drizzle sesame oil, toss in half the scallions → taste & adjust (add more soy or gochujang).  \n",
      "7. Eggs: In a separate pan, fry eggs sunny‑side up (runny yolk recommended).  \n",
      "8. Serve: Divide rice into bowls, top each with an egg, remaining scallion & sesame seeds.\n",
      "\n",
      "Approx. time: 15 min  \n",
      "\n",
      "2. Beef Bulgogi (Korean‑Style Marinated Beef)  \n",
      "Ingredients (2–3 servings)  \n",
      " • 300 g (10 oz) thinly sliced beef sirloin or ribeye  \n",
      " • 3 Tbsp soy sauce  \n",
      " • 2 Tbsp brown sugar or honey  \n",
      " • 1 Tbsp mirin or dry white wine (optional)  \n",
      " • 2 cloves garlic, minced  \n",
      " • 1 tsp fresh ginger, grated  \n",
      " • 1 Tbsp sesame oil  \n",
      " • Pinch black pepper  \n",
      " • ½ small onion, thinly sliced  \n",
      " • 1 small carrot, thin matchsticks (optional)  \n",
      " • 3 scallions, cut into 2 cm pieces  \n",
      " • 1 tsp toasted sesame seeds  \n",
      "\n",
      "Steps  \n",
      "1. Marinade: In a bowl combine soy sauce, sugar, mirin, garlic, ginger, sesame oil and pepper. Add beef + onion + carrot; mix well. Marinate 20–30 min (or up to 2 hr in fridge).  \n",
      "2. Cook: Heat a skillet or grill‑pan over high heat. Add beef mixture in a single layer (do in batches to avoid steaming). Sear 1–2 min per side until just cooked.  \n",
      "3. Toss in scallions for 30 sec.  \n",
      "4. Serve: Plate with steamed rice, lettuce leaves for wraps, kimchi and ssamjang (optional).\n",
      "\n",
      "Approx. time: 45 min (incl. marinating)  \n",
      "\n",
      "3. Bibimbap (Mixed Rice Bowl)  \n",
      "Ingredients (2 servings)  \n",
      " • 2 cups cooked short‑grain rice  \n",
      " • 100 g (3½ oz) ground beef or thinly sliced beef (optional)  \n",
      " • ½ zucchini, julienned  \n",
      " • 1 carrot, julienned  \n",
      " • 1 cup spinach or bean sprouts  \n",
      " • 4–5 shiitake mushrooms, sliced  \n",
      " • 2 Tbsp soy sauce, divided  \n",
      " • 1 tsp sugar  \n",
      " • 1 clove garlic, minced (divided)  \n",
      " • 1 tsp sesame oil, divided  \n",
      " • Neutral oil for sautéing  \n",
      " • 2 eggs  \n",
      " • Gochujang to taste (start with 1 Tbsp per bowl)  \n",
      " • Sesame seeds, scallion slices  \n",
      "\n",
      "Steps  \n",
      "1. Prep vegetables in separate bowls:  \n",
      "   a. Spinach or sprouts: Blanch 1 min in boiling water (salted), drain, squeeze dry, toss with ½ tsp sesame oil, ½ tsp soy, ½ tsp minced garlic.  \n",
      "   b. Zucchini & carrot: Sauté each in 1 tsp oil over medium heat, season with a pinch of salt, set aside.  \n",
      "   c. Mushrooms: Sauté in 1 tsp oil + ½ tsp soy + a pinch sugar until soft.  \n",
      "   d. Beef (if using): Marinate 1 Tbsp soy + ½ tsp sugar + ½ tsp sesame oil + ½ tsp garlic, then sauté until just cooked.  \n",
      "2. Rice: Warm bowls of rice.  \n",
      "3. Assemble: Arrange each veggie/meat in sections on rice.  \n",
      "4. Egg: Fry eggs sunny‑side up, place on top.  \n",
      "5. Sauce & Serve: Dollop gochujang, sprinkle sesame seeds & scallions. To eat, mix everything thoroughly.\n",
      "\n",
      "Approx. time: 30 min  \n",
      "\n",
      "Tips for all dishes  \n",
      "• Adjust spice: add more/less gochujang or fresh chilies.  \n",
      "• Garnish: fresh cucumber slices or a side of store‑bought kimchi.  \n",
      "• Equipment: a good nonstick pan or well‑seasoned cast iron makes stir‑fries easier.  \n",
      "\n",
      "Enjoy your Korean feast! Let me know if you’d like more details or another recipe (seafood pancakes, tteokbokki, etc.).Here’s how you can turn each of these into a vegetarian (or even vegan) dish, with minimal tweaks to the original recipes.  \n",
      "\n",
      "1. Kimchi Fried Rice  \n",
      "   • Kimchi: Most store‑bought kimchis use fish sauce or shrimp paste. Look for “vegan kimchi” at Korean markets or make your own by substituting the fish ingredients with a little crushed nori (seaweed) and a splash of soy sauce or tamari in your chili paste.  \n",
      "   • Eggs (optional): If you eat eggs, keep the sunny‑side‑up as written. For a vegan version, crumble firm tofu and sauté it in a little oil, a pinch of turmeric for color, plus salt and black salt (kala namak) for that eggy aroma. Stir it in at the very end or serve on top.  \n",
      "\n",
      "2. “Mushroom Bulgogi” (in place of Beef Bulgogi)  \n",
      "   Ingredients (2–3 servings)  \n",
      "    • 300 g (10 oz) king oyster mushrooms (or shiitakes + cremini), thinly sliced lengthwise into “scallops”  \n",
      "    • 3 Tbsp soy sauce  \n",
      "    • 2 Tbsp brown sugar or maple syrup  \n",
      "    • 1 Tbsp mirin or dry white wine (optional)  \n",
      "    • 2 cloves garlic, minced  \n",
      "    • 1 tsp fresh ginger, grated  \n",
      "    • 1 Tbsp sesame oil  \n",
      "    • Pinch black pepper  \n",
      "    • ½ small onion, thinly sliced  \n",
      "    • 3 scallions, cut into 2 cm pieces  \n",
      "    • 1 tsp toasted sesame seeds  \n",
      "\n",
      "   Steps  \n",
      "    1. Marinade: Whisk together soy sauce, sugar (or syrup), mirin, garlic, ginger, sesame oil and pepper. Toss in the mushroom slices and onion. Marinate 15–20 minutes.  \n",
      "    2. Cook: Heat a skillet or grill pan over high heat. Add the mushrooms in a single layer, sear 2 minutes per side—do batches if needed.  \n",
      "    3. In the last 30 sec, stir in scallions, then sprinkle with sesame seeds.  \n",
      "    4. Serve with rice, lettuce leaves and gochujang/ssamjang on the side.  \n",
      "\n",
      "3. Vegetarian Bibimbap  \n",
      "   • Omit the ground beef. Instead:  \n",
      "     – Sauté 150 g crumbled firm tofu (or tempeh) in 1 Tbsp soy sauce, ½ tsp sugar, ½ tsp sesame oil and ½ tsp garlic until lightly browned.  \n",
      "     – Or use the same mushroom mix from above: thinly sliced king oysters, marinated briefly in 1 Tbsp soy + ½ tsp sugar + ½ tsp sesame oil, then sauté.  \n",
      "   • Eggs: Keep them sunny‑side‑up if you eat eggs. For vegan, sprinkle fried tofu cubes or pan‑crispy chickpeas on top instead.  \n",
      "   • All other veg prep stays the same—zucchini, carrot, spinach/beansprouts, mushrooms.  \n",
      "\n",
      "General tips  \n",
      " • Sesame oil, soy and garlic do the heavy lifting on flavor—your swaps just need to soak those in.  \n",
      " • If you need extra umami, add a little mushroom stock powder or a dash of vegan Worcestershire.  \n",
      " • Garnish with plenty of toasted sesame seeds and scallions, and serve with gochujang on the side so everyone can adjust the heat.  \n",
      "\n",
      "Enjoy your meat‑free Korean feast!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here’s how you can turn each of these into a vegetarian (or even vegan) dish, with minimal tweaks to the original recipes.  \\n\\n1. Kimchi Fried Rice  \\n   • Kimchi: Most store‑bought kimchis use fish sauce or shrimp paste. Look for “vegan kimchi” at Korean markets or make your own by substituting the fish ingredients with a little crushed nori (seaweed) and a splash of soy sauce or tamari in your chili paste.  \\n   • Eggs (optional): If you eat eggs, keep the sunny‑side‑up as written. For a vegan version, crumble firm tofu and sauté it in a little oil, a pinch of turmeric for color, plus salt and black salt (kala namak) for that eggy aroma. Stir it in at the very end or serve on top.  \\n\\n2. “Mushroom Bulgogi” (in place of Beef Bulgogi)  \\n   Ingredients (2–3 servings)  \\n    • 300\\xa0g (10\\xa0oz) king oyster mushrooms (or shiitakes + cremini), thinly sliced lengthwise into “scallops”  \\n    • 3\\xa0Tbsp soy sauce  \\n    • 2\\xa0Tbsp brown sugar or maple syrup  \\n    • 1\\xa0Tbsp mirin or dry white wine (optional)  \\n    • 2 cloves garlic, minced  \\n    • 1\\xa0tsp fresh ginger, grated  \\n    • 1\\xa0Tbsp sesame oil  \\n    • Pinch black pepper  \\n    • ½ small onion, thinly sliced  \\n    • 3 scallions, cut into 2\\xa0cm pieces  \\n    • 1\\xa0tsp toasted sesame seeds  \\n\\n   Steps  \\n    1. Marinade: Whisk together soy sauce, sugar (or syrup), mirin, garlic, ginger, sesame oil and pepper. Toss in the mushroom slices and onion. Marinate 15–20\\xa0minutes.  \\n    2. Cook: Heat a skillet or grill pan over high heat. Add the mushrooms in a single layer, sear 2\\xa0minutes per side—do batches if needed.  \\n    3. In the last 30\\xa0sec, stir in scallions, then sprinkle with sesame seeds.  \\n    4. Serve with rice, lettuce leaves and gochujang/ssamjang on the side.  \\n\\n3. Vegetarian Bibimbap  \\n   • Omit the ground beef. Instead:  \\n     – Sauté 150\\xa0g crumbled firm tofu (or tempeh) in 1\\xa0Tbsp soy sauce, ½\\xa0tsp sugar, ½\\xa0tsp sesame oil and ½\\xa0tsp garlic until lightly browned.  \\n     – Or use the same mushroom mix from above: thinly sliced king oysters, marinated briefly in 1\\xa0Tbsp soy + ½\\xa0tsp sugar + ½\\xa0tsp sesame oil, then sauté.  \\n   • Eggs: Keep them sunny‑side‑up if you eat eggs. For vegan, sprinkle fried tofu cubes or pan‑crispy chickpeas on top instead.  \\n   • All other veg prep stays the same—zucchini, carrot, spinach/beansprouts, mushrooms.  \\n\\nGeneral tips  \\n • Sesame oil, soy and garlic do the heavy lifting on flavor—your swaps just need to soak those in.  \\n • If you need extra umami, add a little mushroom stock powder or a dash of vegan Worcestershire.  \\n • Garnish with plenty of toasted sesame seeds and scallions, and serve with gochujang on the side so everyone can adjust the heat.  \\n\\nEnjoy your meat‑free Korean feast!', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'o4-mini-2025-04-16'}, id='run-2515a9da-4ae9-48e8-bd72-9ae9f6078bac-0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "#normal recipes\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class international chef. You create easy-to-follow recipes for any type of cuising with easy-to-find ingredients.\"),\n",
    "    (\"human\", \"I want to cook {country} food.\")\n",
    "])\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "#vegetarian recipes\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized in making traditional recipes vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative, just say you don't know how to replace it.\"),\n",
    "    (\"human\", \"{recipe}\")\n",
    "])\n",
    "veg_chef_chain = veg_chef_prompt | chat\n",
    "\n",
    "#mixing two chains\n",
    "final_chain = {\"recipe\":chef_chain} | veg_chef_chain\n",
    "\n",
    "#calling OpenAI API\n",
    "final_chain.invoke({\n",
    "    \"country\":\"korean\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9cda6",
   "metadata": {},
   "source": [
    "## 4. MODEL IO\n",
    "* <b>Modules</b>: Model I/O, Retrieval, Chains, Agents, Memory, Callbacks\n",
    "* <b>Model I/O</b> : Interface with language models<br/>\n",
    "   - prompts, language models, output parsers\n",
    "* <b>Retrieval</b>: 외부 데이터로 접근하여 이를 모델에 제공\n",
    "   - document loaders, transformers, text embedding, vector stores, retrievers\n",
    "* <b>Agents</b>: AI가 독립적으로 작동할 수 있게 함.\n",
    "* <b>Callbacks</b>: Log and stream intermediate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5000c-0b23-4a09-87a1-e0c0cb53fb0e",
   "metadata": {},
   "source": [
    "### 4.1 FewShotPromptTemplate\n",
    "* 모델에게 예제를 주는 것과 같음 (대답 포맷 등). 답변의 형식을 구구절절 설명하는 것보다 더 빠르고 정확할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b862f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PromptTemplate을 이렇게 만들 수도 있음 (그냥 알고 있으라고)\n",
    "t = PromptTemplate(\n",
    "    template = \"What is the capital of {country}?\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "#same as PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "t.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d548783a-6048-4a57-98ba-c85e9a0b07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: The capital is Warsaw, the language is Polish, and the food includes pierogi, kielbasa, and bigos."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI: The capital is Warsaw, the language is Polish, and the food includes pierogi, kielbasa, and bigos.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-400ce6e5-c9b3-44c7-97c8-f8ae7dbe8190-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"question\":\"What do you know about France?\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"question\":\"What do you know about Korea?\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"question\":\"What do you know about Italy?\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"question\":\"What do you know about Japan?\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\":\"Poland\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12904b-ca98-44bf-ae83-3f6fba046d3e",
   "metadata": {},
   "source": [
    "### 4.2 FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "666455a4-2da9-4b98-acf4-a3b0daf10e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital is Washington D.C., \n",
      "language is English, \n",
      "food is burgers and apple pie."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='capital is Washington D.C., \\nlanguage is English, \\nfood is burgers and apple pie.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-343e758b-b342-4748-9a4a-df05eab6528d-0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate # type: ignore\n",
    "from langchain.prompts import ChatPromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"country\":\"France\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"country\":\"Korea\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"country\":\"Italy\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"country\":\"Japan\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\",\"{answer}\")\n",
    "])\n",
    "\n",
    "fewshot_example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a geography expert\"),\n",
    "    fewshot_example_prompt, #이전의 모든 example들\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke(\"USA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c8971-6202-4892-bdca-671cfea9d662",
   "metadata": {},
   "source": [
    "### 4.3 LengthBasedExampleSelector\n",
    "* dynamic하게 example들을 선택 -> 셀 수 없을 정도로 많은 example들이 있을 수 있음. 전부 모델한테 줄 수는 없어.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e901f2a8-4d44-4ae7-bd56-c5dcd3210c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:What do you know about France?\\nAI:capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\\n\\nHuman: What do you know about Netherlands?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"question\":\"What do you know about France?\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"question\":\"What do you know about Korea?\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"question\":\"What do you know about Italy?\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"question\":\"What do you know about Japan?\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    max_length=30\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    example_selector = example_selector, #전에는 그냥 모든 example들을 넣었음\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d591a5-057e-403a-80f4-fda904130e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:What do you know about Italy?\\nAI:capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\\n\\nHuman: What do you know about Netherlands?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 랜덤하게 example들을 선택\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    "    )\n",
    "\n",
    "examples = [\n",
    "    {\"question\":\"What do you know about France?\", \n",
    "    \"answer\":\"capital is Paris, \\nlanguage is French, \\nfood is Bœuf bourguignon and onion soup.\"},\n",
    "    {\"question\":\"What do you know about Korea?\", \n",
    "    \"answer\":\"capital is Seoul, \\nlanguage is Korean, \\nfood is Kimchi and Bibimbap.\"},\n",
    "    {\"question\":\"What do you know about Italy?\", \n",
    "    \"answer\":\"capital is Rome, \\nlanguage is Italian, \\nfood is pasta and pizza.\"},\n",
    "    {\"question\":\"What do you know about Japan?\", \n",
    "    \"answer\":\"capital is Tokyo, \\nlanguage is Japanese, \\nfood is sushi and sashimi.\"}\n",
    "]\n",
    "\n",
    "# 나만의 example selector를 만들 수 있다는 간단한 예시\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example): #ExampleSelector 내부에 반드시 있어야 함 (없으면 에러)\n",
    "        return super().add_example(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)] #하나의 example만 선택해서 리턴\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human:{question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    example_selector = example_selector, #전에는 그냥 모든 example들을 넣었음\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb2f21-4937-4a45-8e82-978e71b5eeeb",
   "metadata": {},
   "source": [
    "### 4.4 Serialization and Composition\n",
    "* serialize(직렬화): 불러오기, 저장 등\n",
    "* compose: 작은 prompt templates들을 결합\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eeec081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of China?\n",
      "What is the capital of China?\n"
     ]
    }
   ],
   "source": [
    "# prompt 불러오기\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "print(prompt.format(country=\"China\"))\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "print(prompt.format(country=\"China\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f77405d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_25744/1575831411.py:52: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  full_prompt = PipelinePromptTemplate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr matey! Me favorite food be grilled fish and salted pork, aye! Nothing like a good meal to keep me strength up on the high seas. Arrrr!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrr matey! Me favorite food be grilled fish and salted pork, aye! Nothing like a good meal to keep me strength up on the high seas. Arrrr!', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'}, id='run-8c1d7fe1-eb46-44e4-911a-06350539baab-0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 개의 prompt를 하나로 합치기\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler # type: ignore\n",
    "from langchain.prompts import PromptTemplate # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}.\n",
    "\"\"\"\n",
    ")\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human:{example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "start=PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "\n",
    "    {example}\n",
    "\n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts=[\n",
    "    (\"intro\",intro),\n",
    "    (\"example\",example),\n",
    "    (\"start\",start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts\n",
    ")\n",
    "\n",
    "chain = full_prompt | chat\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"character\":\"Pirate\",\n",
    "        \"example_question\":\"What is your location?\",\n",
    "        \"example_answer\":\"Arrrrg! That is a scret!! Arg arrrg!\",\n",
    "        \"question\": \"What is your favorite food?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56f1d0",
   "metadata": {},
   "source": [
    "### 4.5 Caching\n",
    "* 질문, 답변을 저장. 중복되는 API 콜을 줄일 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d374944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ingredients:\\n- 2 cups cooked rice\\n- 1 cup kimchi, chopped\\n- 1/2 cup kimchi juice\\n- 1/2 cup diced pork belly or bacon\\n- 1/2 onion, diced\\n- 2 cloves garlic, minced\\n- 1 tbsp gochujang (Korean red chili paste)\\n- 1 tbsp soy sauce\\n- 1 tbsp sesame oil\\n- 1 green onion, chopped\\n- 1 fried egg (optional)\\n- Sesame seeds for garnish\\n\\nInstructions:\\n1. Heat a large skillet over medium heat and add the diced pork belly or bacon. Cook until browned and crispy.\\n2. Add the diced onion and minced garlic to the skillet and cook until softened.\\n3. Add the chopped kimchi and cook for a few minutes until heated through.\\n4. Stir in the gochujang, soy sauce, and kimchi juice. Mix well.\\n5. Add the cooked rice to the skillet and stir to combine with the kimchi mixture.\\n6. Drizzle with sesame oil and continue to cook for a few more minutes, stirring occasionally.\\n7. Remove from heat and garnish with chopped green onion and sesame seeds.\\n8. Serve hot with a fried egg on top, if desired. Enjoy your delicious kimchi fried rice!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 270, 'prompt_tokens': 21, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BP6MpaJDcZEoFojgHlPwgOgCxGorB', 'finish_reason': 'stop', 'logprobs': None}, id='run-540e0871-6706-4956-8ea1-62e82cec7170-0', usage_metadata={'input_tokens': 21, 'output_tokens': 270, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "\n",
    "# set_llm_cache(InMemoryCache()) #이제부터 모든 response가 메모리에 저장됨. 컴퓨터를 재시동하면 없어짐.\n",
    "set_llm_cache(SQLiteCache(\"cache.db\")) #sqlite 데이터베이스 생성. 여기에 response를 저장함.\n",
    "set_debug(True) #로그를 보여줌. 나중에 체인 작업을 할 때 매우 유용\n",
    "\n",
    "chat=ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "chat.invoke(\"Give me a recipe of 김치볶음밥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50fccc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ingredients:\\n- 2 cups cooked rice\\n- 1 cup kimchi, chopped\\n- 1/2 cup kimchi juice\\n- 1/2 cup diced pork belly or bacon\\n- 1/2 onion, diced\\n- 2 cloves garlic, minced\\n- 1 tbsp gochujang (Korean red chili paste)\\n- 1 tbsp soy sauce\\n- 1 tbsp sesame oil\\n- 1 green onion, chopped\\n- 1 fried egg (optional)\\n- Sesame seeds for garnish\\n\\nInstructions:\\n1. Heat a large skillet over medium heat and add the diced pork belly or bacon. Cook until browned and crispy.\\n2. Add the diced onion and minced garlic to the skillet and cook until softened.\\n3. Add the chopped kimchi and cook for a few minutes until heated through.\\n4. Stir in the gochujang, soy sauce, and kimchi juice. Mix well.\\n5. Add the cooked rice to the skillet and stir to combine with the kimchi mixture.\\n6. Drizzle with sesame oil and continue to cook for a few more minutes, stirring occasionally.\\n7. Remove from heat and garnish with chopped green onion and sesame seeds.\\n8. Serve hot with a fried egg on top, if desired. Enjoy your delicious kimchi fried rice!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 270, 'prompt_tokens': 21, 'total_tokens': 291, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BP6MpaJDcZEoFojgHlPwgOgCxGorB', 'finish_reason': 'stop', 'logprobs': None}, id='run-540e0871-6706-4956-8ea1-62e82cec7170-0', usage_metadata={'input_tokens': 21, 'output_tokens': 270, 'total_tokens': 291, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"Give me a recipe of 김치볶음밥\") #똑같은 질문을 두 번째 함 -> 답변하는데 0초 소요. 메모리에서 들고 왔으니까."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd09d87",
   "metadata": {},
   "source": [
    "### 4.6 Serialization\n",
    "* 모델을 저장 & 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d7bdc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 430\n",
      "\tPrompt Tokens: 22\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 408\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0006230000000000001\n",
      "content='Kimchi is a traditional Korean side dish made of fermented vegetables, most commonly napa cabbage and Korean radish. Here is a simple recipe to make kimchi at home:\\n\\nIngredients:\\n- 1 napa cabbage\\n- 1 Korean radish\\n- 1/2 cup of coarse sea salt\\n- 1/4 cup of Korean red pepper flakes (gochugaru)\\n- 1 tablespoon of minced garlic\\n- 1 tablespoon of minced ginger\\n- 1 tablespoon of fish sauce\\n- 1 tablespoon of sugar\\n- 2 green onions, chopped\\n- 1 carrot, julienned\\n\\nInstructions:\\n1. Cut the napa cabbage in half lengthwise, then cut each half into quarters. Remove the core and chop the cabbage into bite-sized pieces. Peel the Korean radish and cut it into thin matchsticks.\\n\\n2. In a large bowl, dissolve the sea salt in water. Add the cabbage and radish, making sure they are fully submerged in the brine. Let it sit for 2-3 hours, tossing occasionally.\\n\\n3. Rinse the cabbage and radish under cold water to remove excess salt. Drain well and set aside.\\n\\n4. In a separate bowl, mix together the red pepper flakes, garlic, ginger, fish sauce, and sugar to make a paste.\\n\\n5. Add the green onions and carrot to the paste, then mix in the cabbage and radish until well combined.\\n\\n6. Pack the kimchi into a clean glass jar, pressing down to remove any air bubbles. Leave some space at the top of the jar as the kimchi will expand during fermentation.\\n\\n7. Cover the jar loosely with a lid and let it sit at room temperature for 1-2 days to ferment. Taste the kimchi daily until it reaches your desired level of fermentation.\\n\\n8. Once the kimchi is ready, store it in the refrigerator to slow down the fermentation process. Enjoy your homemade kimchi as a side dish or in various Korean dishes!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 408, 'prompt_tokens': 22, 'total_tokens': 430, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPC2wfwLPNVikfQm1t1PxBictdWaF', 'finish_reason': 'stop', 'logprobs': None} id='run-9212a912-151a-4e3c-830d-ca238fa739f1-0' usage_metadata={'input_tokens': 22, 'output_tokens': 408, 'total_tokens': 430, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# 한 번 API 콜을 할 떄 소요되는 비용 체크\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "from langchain.callbacks import get_openai_callback # type: ignore\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    kimchi_recipe = chat.invoke(\"Tell me how I can make Kimchi(겉절이)\")\n",
    "    print(usage)\n",
    "    print(usage.total_cost) #가격에 대해서만 알고 싶다면\n",
    "    print(usage.prompt_tokens) #내가 모델에 건네준 토큰\n",
    "    print(usage.completion_tokens) #모델이 생성한 토큰\n",
    "\n",
    "print(kimchi_recipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0dd993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장하기\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_completion_tokens=500, #말 많은거 싫어\n",
    "    temperature=0.1 #너무 창의적인거 싫어\n",
    ")\n",
    "# chat.save(\"model.json\") #model.json 파일이 생성됨 -> 더 이상 지원하지 않음. joblib 혹은 pickle을 사용해야 함.\n",
    "\n",
    "# #모델 불러오기\n",
    "from langchain.llms.loading import load_llm\n",
    "# chat = load_llm(\"model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504338e9",
   "metadata": {},
   "source": [
    "## 5. MEMORY\n",
    "* OpenAI의 Chat은 기본적으로 memory를 제공하지 않음 (stateless)\n",
    "* 랭체인을 이용해서 implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c695340",
   "metadata": {},
   "source": [
    "### 5.0 ConversationBufferMemory\n",
    "* <b>Conversation Buffer Memory</b>: 대화 전체를 저장. prompt가 길어짐 -> 비싸!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4a233ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='How are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True) #return_messages=False를 주면 메시지 객체가 아니라 문자열을 반환\n",
    "\n",
    "memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285bce5",
   "metadata": {},
   "source": [
    "### 5.1 ConversationBufferWindowMemory\n",
    "* <b>Conversation Buffer Window Memory</b>: 최근 n개의 대화를 저장 -> 최근 대화만 기억할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9bbf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyoon/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_88588/1223011226.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='3', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='5', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='5', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(\"1\",\"1\") #이 부분은 버려짐\n",
    "add_message(\"2\",\"2\")\n",
    "add_message(\"3\",\"3\")\n",
    "add_message(\"4\",\"4\")\n",
    "add_message(\"5\",\"5\")\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aef977",
   "metadata": {},
   "source": [
    "### 5.2 ConversationSummaryMemory\n",
    "* <b>Conversation Summary Memory</b>: llm을 이용. 대화가 길어질수록 유용해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0ed9388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces themselves as Boyon from Paris. The AI responds with enthusiasm at the information shared, expressing a desire to visit the beautiful city of Paris.'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi! I'm Boyon, I live in Paris.\", \"Wow that is so cool!\")\n",
    "add_message(\"Paris is very beautiful.\", \"I wish I could go\")\n",
    "get_history() #진행된 대화가 별로 없음 -> 초반엔 대화보다 summary가 더 긺."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec362e",
   "metadata": {},
   "source": [
    "### 5.3 ConversationSummaryBufferMemory\n",
    "* <b>Conversation Summary Buffer Memory</b>: max_token_limit에 달하면, 최근 메시지를 제외한 메시지들을 summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65c1ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_25744/1068426509.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='Boyon introduces themselves and mentions they live in Paris. The AI expresses admiration for this fact. Boyon agrees, stating that Paris is very beautiful.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='I wish I could go', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=20, #최근 max_token_limit개 토큰의 interactions를 제외하고 summarize\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi! I'm Boyon, I live in Paris.\", \"Wow that is so cool!\")\n",
    "add_message(\"Paris is very beautiful.\", \"I wish I could go\")\n",
    "get_history() #진행된 대화가 별로 없음 -> 초반엔 대화보다 summary가 더 긺."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fe54f",
   "metadata": {},
   "source": [
    "### 5.4 ConversationKGMemory\n",
    "* <b>Conversational Knowledge Graph Memory</b>: 대화 중인 entity의 knowledge graph를 만듦.\n",
    "* 대화에서 추출한 지식(triple: 주어-동사-목적어) 를 그래프 형태로 저장.\n",
    "* 예: \"Alice lives in Paris\" → (Alice, lives in, Paris)\n",
    "* 지식 기반 챗봇이나 엔티티 추론 등에 유용.\n",
    "* 일반 대화보다는 정보 추출이 중요한 상황에서 사용.\n",
    "* <code>.save_context()</code>: 입력/출력 내용을 분석해 triple로 바꾸기 위해 LLM을 호출\n",
    "* <code>.load_memory_variables</code>: keyword 기반으로 context를 반환. 실제 응답은 하지 않고, 문맥을 제공. API 호출 x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bf069c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': [SystemMessage(content='On Boyon: Boyon lives in Paris.', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory=ConversationKGMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(\"Hello! I'm Boyoon.\", \"Hi Boyoon! How can I help you today?\")\n",
    "add_message(\"What's your name?\", \"I'm an AI assistant created to help you.\")\n",
    "add_message(\"I like pizza.\", \"Pizza is delicious! What's your favorite topping?\")\n",
    "print(memory.load_memory_variables({\"input\":\"Who is Boyoon?\"})) \n",
    "#entity의 knowledge graph를 만들어 요약본을 생성, 메모리에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d578183",
   "metadata": {},
   "source": [
    "### 5.5 Memory on LLMChain\n",
    "* LLM chain: off-the-shelf chain. 일반적인 목적. 빠르게 시작할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bc1cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_88588/2986166140.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n",
      "/var/folders/n3/6fbssxjx1x7bxcnvp_nn7g3r0000gn/T/ipykernel_88588/2986166140.py:21: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "Python(14759) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    \n",
      "    Human: My name is Boyoon.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Nice to meet you, Boyoon! How can I assist you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    Human: My name is Boyoon.\n",
      "AI: Nice to meet you, Boyoon! How can I assist you today?\n",
      "    Human: I live in Paris since 7 years ago, but I was born and raised in Seoul.\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's interesting! How do you like living in Paris compared to Seoul?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    System: The human introduces themselves as Boyoon. The AI responds by greeting Boyoon and asking how it can assist them today.\n",
      "Human: I live in Paris since 7 years ago, but I was born and raised in Seoul.\n",
      "AI: That's interesting! How do you like living in Paris compared to Seoul?\n",
      "    Human: While I was traveling, a stranger asked me where I'm from. How do I answer to that?\n",
      "    You: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You can simply answer by saying \"I'm originally from Seoul, but I currently live in Paris.\" This way, you acknowledge both your hometown and your current place of residence.\n"
     ]
    }
   ],
   "source": [
    "# LLMChain에 우리가 가진 메모리 넣기\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key = \"chat_history\" #template안에 넣을 history의 위치를 알려줌\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    {chat_history}\n",
    "    Human: {question}\n",
    "    You: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True, #chain의 프롬프트 로그들을 확인할 수 있음\n",
    ")\n",
    "\n",
    "print(chain.predict(question=\"My name is Boyoon.\"))\n",
    "print(chain.predict(question=\"I live in Paris since 7 years ago, but I was born and raised in Seoul.\"))\n",
    "print(chain.predict(question=\"While I was traveling, a stranger asked me where I'm from. How do I answer to that?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40038a5e",
   "metadata": {},
   "source": [
    "### 5.6 Chat Based Memory\n",
    "* memory.load_memory_variables({})가 문자열이 아니라 메시지 객체를 반환하게 합시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263350cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Boyoon.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Boyoon! How can I assist you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "Human: My name is Boyoon.\n",
      "AI: Hello Boyoon! How can I assist you today?\n",
      "Human: I live in Paris since 7 years ago, but I was born and raised in Seoul.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "That's interesting! It sounds like you have a multicultural background. How are you finding life in Paris compared to Seoul?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human.\n",
      "System: The human introduces themselves as Boyoon and mentions living in Paris for the past 7 years, but being born and raised in Seoul. The AI greets Boyoon and asks how it can assist.\n",
      "AI: That's interesting! It sounds like you have a multicultural background. How are you finding life in Paris compared to Seoul?\n",
      "Human: What's my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Your name is Boyoon. How can I assist you today, Boyoon?\n"
     ]
    }
   ],
   "source": [
    "# LLMChain에 우리가 가진 메모리 넣기\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), #누가 보냈는지 알 수 없고 예측하기 어려운 양의 메시지를 넣을 수 있음\n",
    "    (\"human\",\"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(chain.predict(question=\"My name is Boyoon.\"))\n",
    "print(chain.predict(question=\"I live in Paris since 7 years ago, but I was born and raised in Seoul.\"))\n",
    "print(chain.predict(question=\"What's my name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620db9a",
   "metadata": {},
   "source": [
    "### 5.7 LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ba640dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'question'] input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10ad8a430>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI talking to a human.'), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]\n",
      "{'question': 'My name is Boyoon'}\n",
      "content='Nice to meet you, Boyoon! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 26, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPw8AVifvIgVO1ZJOPgfKSzpRvVKp', 'finish_reason': 'stop', 'logprobs': None} id='run-d289ca97-a5de-48f3-a674-c7e6bd54b8f0-0' usage_metadata={'input_tokens': 26, 'output_tokens': 16, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "{'question': 'What is my name?'}\n",
      "content='Your name is Boyoon.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 54, 'total_tokens': 61, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BPw8CIWUgLLo5ouoTXbu6FsOpkMTx', 'finish_reason': 'stop', 'logprobs': None} id='run-8df1fb25-ad53-437d-ab88-789fc68c4098-0' usage_metadata={'input_tokens': 54, 'output_tokens': 7, 'total_tokens': 61, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm=ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    memory_key = \"history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), #누가 보냈는지 알 수 없고 예측하기 어려운 양의 메시지를 넣을 수 있음\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "    print(_)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm \n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\":question})\n",
    "    memory.save_context(\n",
    "        {\"input\":question}, \n",
    "        {\"output\":result.content}\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"My name is Boyoon\")\n",
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43116876",
   "metadata": {},
   "source": [
    "## 6 RAG\n",
    "* RAG: Retrieval Augmented Generation. 모델이 잘 모르는 정보를 제공하고, 그를 기반으로 우리 질문에 대답하게 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5082c",
   "metadata": {},
   "source": [
    "### 6.1 Data Loaders and Splitters\n",
    "* <b>Retrieval</b>: langchain의 모듈\n",
    "* <b>embedding</b>: 텍스트를 숫자로 변환\n",
    "* <b>loader</b>: 소스에서 데이터를 추출라고 랭체인에게 가져다 주는 코드\n",
    "* <b>document</b>: 텍스트 덩어리. 너무 크면 splitter로 분리 -> 질문에 답변하는 데 필요한 부분만 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "708efe7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './files/chapter_one.txt'}, page_content='Part One'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='1'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Inside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Outside, even through the shut window-pane, the world looked cold. Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere. The blackmoustachio'd face gazed down from every\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The blackmoustachio'd face gazed down from every commanding corner. There was one on the house-front immediately opposite. BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"YOU, the caption said, while the dark eyes looked deep into Winston's own. Down at streetlevel another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='in the wind, alternately covering and uncovering the single word INGSOC. In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"for an instant like a bluebottle, and darted away again with a curving flight. It was the police patrol, snooping into people's windows. The patrols did not matter, however. Only the Thought Police\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='did not matter, however. Only the Thought Police mattered.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Behind Winston's back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan. The telescreen received and transmitted simultaneously.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='received and transmitted simultaneously. Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard. There was of course no way of knowing whether you were being watched at any'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of knowing whether you were being watched at any given moment. How often, or on what system, the Thought Police plugged in on any individual wire was guesswork. It was even conceivable that they'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='was guesswork. It was even conceivable that they watched everybody all the time. But at any rate they could plug in your wire whenever they wanted to. You had to live -- did live, from habit that'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='to. You had to live -- did live, from habit that became instinct -- in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston kept his back turned to the telescreen. It was safer, though, as he well knew, even a back can be revealing. A kilometre away the Ministry of Truth, his place of work, towered vast and white'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Truth, his place of work, towered vast and white above the grimy landscape. This, he thought with a sort of vague distaste -- this was London, chief city of Airstrip One, itself the third most'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='chief city of Airstrip One, itself the third most populous of the provinces of Oceania. He tried to squeeze out some childhood memory that should tell him whether London had always been quite like'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='him whether London had always been quite like this. Were there always these vistas of rotting nineteenth-century houses, their sides shored up with baulks of timber, their windows patched with'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='with baulks of timber, their windows patched with cardboard and their roofs with corrugated iron, their crazy garden walls sagging in all directions? And the bombed sites where the plaster dust'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='And the bombed sites where the plaster dust swirled in the air and the willow-herb straggled over the heaps of rubble; and the places where the bombs had cleared a larger patch and there had sprung'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='had cleared a larger patch and there had sprung up sordid colonies of wooden dwellings like chicken-houses? But it was no use, he could not remember: nothing remained of his childhood except a series'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='nothing remained of his childhood except a series of bright-lit tableaux occurring against no background and mostly unintelligible.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The Ministry of Truth -- Minitrue, in Newspeak -- was startlingly different from any other object in sight. It was an enormous pyramidal structure of glittering white concrete, soaring up, terrace'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of glittering white concrete, soaring up, terrace after terrace, 300 metres into the air. From where Winston stood it was just possible to read, picked out on its white face in elegant lettering, the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='out on its white face in elegant lettering, the three slogans of the Party:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='WAR IS PEACE\\n\\nFREEDOM IS SLAVERY\\n\\nIGNORANCE IS STRENGTH'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The Ministry of Truth contained, it was said, three thousand rooms above ground level, and corresponding ramifications below. Scattered about London there were just three other buildings of similar'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='there were just three other buildings of similar appearance and size. So completely did they dwarf the surrounding architecture that from the roof of Victory Mansions you could see all four of them'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Victory Mansions you could see all four of them simultaneously. They were the homes of the four Ministries between which the entire apparatus of government was divided. The Ministry of Truth, which'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='was divided. The Ministry of Truth, which concerned itself with news, entertainment, education, and the fine arts. The Ministry of Peace, which concerned itself with war. The Ministry of Love, which'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='itself with war. The Ministry of Love, which maintained law and order. And the Ministry of Plenty, which was responsible for economic affairs. Their names, in Newspeak: Minitrue, Minipax, Miniluv,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='names, in Newspeak: Minitrue, Minipax, Miniluv, and Miniplenty.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The Ministry of Love was the really frightening one. There were no windows in it at all. Winston had never been inside the Ministry of Love, nor within half a kilometre of it. It was a place'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='nor within half a kilometre of it. It was a place impossible to enter except on official business, and then only by penetrating through a maze of barbed-wire entanglements, steel doors, and hidden'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='entanglements, steel doors, and hidden machine-gun nests. Even the streets leading up to its outer barriers were roamed by gorilla-faced guards in black uniforms, armed with jointed truncheons.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston turned round abruptly. He had set his features into the expression of quiet optimism which it was advisable to wear when facing the telescreen. He crossed the room into the tiny kitchen. By'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='He crossed the room into the tiny kitchen. By leaving the Ministry at this time of day he had sacrificed his lunch in the canteen, and he was aware that there was no food in the kitchen except a hunk'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"there was no food in the kitchen except a hunk of dark-coloured bread which had got to be saved for tomorrow's breakfast. He took down from the shelf a bottle of colourless liquid with a plain white\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='a bottle of colourless liquid with a plain white label marked VICTORY GIN. It gave off a sickly, oily smell, as of Chinese ricespirit. Winston poured out nearly a teacupful, nerved himself for a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='out nearly a teacupful, nerved himself for a shock, and gulped it down like a dose of medicine.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Instantly his face turned scarlet and the water ran out of his eyes. The stuff was like nitric acid, and moreover, in swallowing it one had the sensation of being hit on the back of the head with a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of being hit on the back of the head with a rubber club. The next moment, however, the burning in his belly died down and the world began to look more cheerful. He took a cigarette from a crumpled'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='cheerful. He took a cigarette from a crumpled packet marked VICTORY CIGARETTES and incautiously held it upright, whereupon the tobacco fell out on to the floor. With the next he was more successful.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the floor. With the next he was more successful. He went back to the living-room and sat down at a small table that stood to the left of the telescreen. From the table drawer he took out a penholder,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='From the table drawer he took out a penholder, a bottle of ink, and a thick, quarto-sized blank book with a red back and a marbled cover.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For some reason the telescreen in the living-room was in an unusual position. Instead of being placed, as was normal, in the end wall, where it could command the whole room, it was in the longer'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='command the whole room, it was in the longer wall, opposite the window. To one side of it there was a shallow alcove in which Winston was now sitting, and which, when the flats were built, had'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='and which, when the flats were built, had probably been intended to hold bookshelves. By sitting in the alcove, and keeping well back, Winston was able to remain outside the range of the telescreen,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='to remain outside the range of the telescreen, so far as sight went. He could be heard, of course, but so long as he stayed in his present position he could not be seen. It was partly the unusual'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='he could not be seen. It was partly the unusual geography of the room that had suggested to him the thing that he was now about to do.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='But it had also been suggested by the book that he had just taken out of the drawer. It was a peculiarly beautiful book. Its smooth creamy paper, a little yellowed by age, was of a kind that had not'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='yellowed by age, was of a kind that had not been manufactured for at least forty years past. He could guess, however, that the book was much older than that. He had seen it lying in the window of a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='that. He had seen it lying in the window of a frowsy little junk-shop in a slummy quarter of the town (just what quarter he did not now remember) and had been stricken immediately by an overwhelming'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"had been stricken immediately by an overwhelming desire to possess it. Party members were supposed not to go into ordinary shops ('dealing on the free market', it was called), but the rule was not\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"market', it was called), but the rule was not strictly kept, because there were various things, such as shoelaces and razor blades, which it was impossible to get hold of in any other way. He had\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='to get hold of in any other way. He had given a quick glance up and down the street and then had slipped inside and bought the book for two dollars fifty. At the time he was not conscious of wanting'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='At the time he was not conscious of wanting it for any particular purpose. He had carried it guiltily home in his briefcase. Even with nothing written in it, it was a compromising possession.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The thing that he was about to do was to open a diary. This was not illegal (nothing was illegal, since there were no longer any laws), but if detected it was reasonably certain that it would be'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='it was reasonably certain that it would be punished by death, or at least by twenty-five years in a forced-labour camp. Winston fitted a nib into the penholder and sucked it to get the grease off.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='penholder and sucked it to get the grease off. The pen was an archaic instrument, seldom used even for signatures, and he had procured one, furtively and with some difficulty, simply because of a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='and with some difficulty, simply because of a feeling that the beautiful creamy paper deserved to be written on with a real nib instead of being scratched with an ink-pencil. Actually he was not used'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='with an ink-pencil. Actually he was not used to writing by hand. Apart from very short notes, it was usual to dictate everything into the speakwrite which was of course impossible for his present'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='which was of course impossible for his present purpose. He dipped the pen into the ink and then faltered for just a second. A tremor had gone through his bowels. To mark the paper was the decisive'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='his bowels. To mark the paper was the decisive act. In small clumsy letters he wrote:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='April 4th, 1984.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='He sat back. A sense of complete helplessness had descended upon him. To begin with, he did not know with any certainty that this was 1984. It must be round about that date, since he was fairly sure'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='round about that date, since he was fairly sure that his age was thirty-nine, and he believed that he had been born in 1944 or 1945; but it was never possible nowadays to pin down any date within a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='possible nowadays to pin down any date within a year or two.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For whom, it suddenly occurred to him to wonder, was he writing this diary? For the future, for the unborn. His mind hovered for a moment round the doubtful date on the page, and then fetched up with'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='date on the page, and then fetched up with a bump against the Newspeak word doublethink. For the first time the magnitude of what he had undertaken came home to him. How could you communicate with'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='came home to him. How could you communicate with the future? It was of its nature impossible. Either the future would resemble the present, in which case it would not listen to him: or it would be'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='case it would not listen to him: or it would be different from it, and his predicament would be meaningless.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For some time he sat gazing stupidly at the paper. The telescreen had changed over to strident military music. It was curious that he seemed not merely to have lost the power of expressing himself,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='to have lost the power of expressing himself, but even to have forgotten what it was that he had originally intended to say. For weeks past he had been making ready for this moment, and it had never'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='making ready for this moment, and it had never crossed his mind that anything would be needed except courage. The actual writing would be easy. All he had to do was to transfer to paper the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='All he had to do was to transfer to paper the interminable restless monologue that had been running inside his head, literally for years. At this moment, however, even the monologue had dried up.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='moment, however, even the monologue had dried up. Moreover his varicose ulcer had begun itching unbearably. He dared not scratch it, because if he did so it always became inflamed. The seconds were'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='so it always became inflamed. The seconds were ticking by. He was conscious of nothing except the blankness of the page in front of him, the itching of the skin above his ankle, the blaring of the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of the skin above his ankle, the blaring of the music, and a slight booziness caused by the gin.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Suddenly he began writing in sheer panic, only imperfectly aware of what he was setting down. His small but childish handwriting straggled up and down the page, shedding first its capital letters and'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the page, shedding first its capital letters and finally even its full stops:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='April 4th, 1984. Last night to the flicks. All war films. One very good one of a ship full of refugees being bombed somewhere in the Mediterranean. Audience much amused by shots of a great huge fat'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Audience much amused by shots of a great huge fat man trying to swim away with a helicopter after him, first you saw him wallowing along in the water like a porpoise, then you saw him through the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='like a porpoise, then you saw him through the helicopters gunsights, then he was full of holes and the sea round him turned pink and he sank as suddenly as though the holes had let in the water,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='as though the holes had let in the water, audience shouting with laughter when he sank. then you saw a lifeboat full of children with a helicopter hovering over it. there was a middle-aged woman'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='hovering over it. there was a middle-aged woman might have been a jewess sitting up in the bow with a little boy about three years old in her arms. little boy screaming with fright and hiding his'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='little boy screaming with fright and hiding his head between her breasts as if he was trying to burrow right into her and the woman putting her arms round him and comforting him although she was blue'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='him and comforting him although she was blue with fright herself, all the time covering him up as much as possible as if she thought her arms could keep the bullets off him. then the helicopter'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"keep the bullets off him. then the helicopter planted a 20 kilo bomb in among them terrific flash and the boat went all to matchwood. then there was a wonderful shot of a child's arm going up up up\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"a wonderful shot of a child's arm going up up up right up into the air a helicopter with a camera in its nose must have followed it up and there was a lot of applause from the party seats but a woman\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='lot of applause from the party seats but a woman down in the prole part of the house suddenly started kicking up a fuss and shouting they didnt oughter of showed it not in front of kids they didnt it'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of showed it not in front of kids they didnt it aint right not in front of kids it aint until the police turned her turned her out i dont suppose anything happened to her nobody cares what the proles'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='happened to her nobody cares what the proles say typical prole reaction they never --'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston stopped writing, partly because he was suffering from cramp. He did not know what had made him pour out this stream of rubbish. But the curious thing was that while he was doing so a totally'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='thing was that while he was doing so a totally different memory had clarified itself in his mind, to the point where he almost felt equal to writing it down. It was, he now realized, because of this'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='it down. It was, he now realized, because of this other incident that he had suddenly decided to come home and begin the diary today.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It had happened that morning at the Ministry, if anything so nebulous could be said to happen.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It was nearly eleven hundred, and in the Records Department, where Winston worked, they were dragging the chairs out of the cubicles and grouping them in the centre of the hall opposite the big'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='them in the centre of the hall opposite the big telescreen, in preparation for the Two Minutes Hate. Winston was just taking his place in one of the middle rows when two people whom he knew by sight,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='rows when two people whom he knew by sight, but had never spoken to, came unexpectedly into the room. One of them was a girl whom he often passed in the corridors. He did not know her name, but he'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the corridors. He did not know her name, but he knew that she worked in the Fiction Department. Presumably -- since he had sometimes seen her with oily hands and carrying a spanner she had some'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='oily hands and carrying a spanner she had some mechanical job on one of the novel-writing machines. She was a bold-looking girl, of about twenty-seven, with thick hair, a freckled face, and swift,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='with thick hair, a freckled face, and swift, athletic movements. A narrow scarlet sash, emblem of the Junior Anti-Sex League, was wound several times round the waist of her overalls, just tightly'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='round the waist of her overalls, just tightly enough to bring out the shapeliness of her hips. Winston had disliked her from the very first moment of seeing her. He knew the reason. It was because of'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='seeing her. He knew the reason. It was because of the atmosphere of hockey-fields and cold baths and community hikes and general clean-mindedness which she managed to carry about with her. He'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='which she managed to carry about with her. He disliked nearly all women, and especially the young and pretty ones. It was always the women, and above all the young ones, who were the most bigoted'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='all the young ones, who were the most bigoted adherents of the Party, the swallowers of slogans, the amateur spies and nosers-out of unorthodoxy. But this particular girl gave him the impression of'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='this particular girl gave him the impression of being more dangerous than most. Once when they passed in the corridor she gave him a quick sidelong glance which seemed to pierce right into him and'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='glance which seemed to pierce right into him and for a moment had filled him with black terror. The idea had even crossed his mind that she might be an agent of the Thought Police. That, it was true,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='agent of the Thought Police. That, it was true, was very unlikely. Still, he continued to feel a peculiar uneasiness, which had fear mixed up in it as well as hostility, whenever she was anywhere'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='as well as hostility, whenever she was anywhere near him.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The other person was a man named O'Brien, a member of the Inner Party and holder of some post so important and remote that Winston had only a dim idea of its nature. A momentary hush passed over the\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"of its nature. A momentary hush passed over the group of people round the chairs as they saw the black overalls of an Inner Party member approaching. O'Brien was a large, burly man with a thick neck\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"O'Brien was a large, burly man with a thick neck and a coarse, humorous, brutal face. In spite of his formidable appearance he had a certain charm of manner. He had a trick of resettling his\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='charm of manner. He had a trick of resettling his spectacles on his nose which was curiously disarming -- in some indefinable way, curiously civilized. It was a gesture which, if anyone had still'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"It was a gesture which, if anyone had still thought in such terms, might have recalled an eighteenth-century nobleman offering his snuffbox. Winston had seen O'Brien perhaps a dozen times in almost\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"had seen O'Brien perhaps a dozen times in almost as many years. He felt deeply drawn to him, and not solely because he was intrigued by the contrast between O'Brien's urbane manner and his\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"contrast between O'Brien's urbane manner and his prize-fighter's physique. Much more it was because of a secretly held belief -- or perhaps not even a belief, merely a hope -- that O'Brien's\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"even a belief, merely a hope -- that O'Brien's political orthodoxy was not perfect. Something in his face suggested it irresistibly. And again, perhaps it was not even unorthodoxy that was written in\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='it was not even unorthodoxy that was written in his face, but simply intelligence. But at any rate he had the appearance of being a person that you could talk to if somehow you could cheat the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='you could talk to if somehow you could cheat the telescreen and get him alone. Winston had never made the smallest effort to verify this guess: indeed, there was no way of doing so. At this moment'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"there was no way of doing so. At this moment O'Brien glanced at his wrist-watch, saw that it was nearly eleven hundred, and evidently decided to stay in the Records Department until the Two Minutes\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='in the Records Department until the Two Minutes Hate was over. He took a chair in the same row as Winston, a couple of places away. A small, sandy-haired woman who worked in the next cubicle to'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='woman who worked in the next cubicle to Winston was between them. The girl with dark hair was sitting immediately behind.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The next moment a hideous, grinding speech, as of some monstrous machine running without oil, burst from the big telescreen at the end of the room. It was a noise that set one's teeth on edge and\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"It was a noise that set one's teeth on edge and bristled the hair at the back of one's neck. The Hate had started.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='As usual, the face of Emmanuel Goldstein, the Enemy of the People, had flashed on to the screen. There were hisses here and there among the audience. The little sandy-haired woman gave a squeak of'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The little sandy-haired woman gave a squeak of mingled fear and disgust. Goldstein was the renegade and backslider who once, long ago (how long ago, nobody quite remembered), had been one of the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='nobody quite remembered), had been one of the leading figures of the Party, almost on a level with Big Brother himself, and then had engaged in counter-revolutionary activities, had been condemned to'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='activities, had been condemned to death, and had mysteriously escaped and disappeared. The programmes of the Two Minutes Hate varied from day to day, but there was none in which Goldstein was not the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"but there was none in which Goldstein was not the principal figure. He was the primal traitor, the earliest defiler of the Party's purity. All subsequent crimes against the Party, all treacheries,\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='crimes against the Party, all treacheries, acts of sabotage, heresies, deviations, sprang directly out of his teaching. Somewhere or other he was still alive and hatching his conspiracies: perhaps'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='alive and hatching his conspiracies: perhaps somewhere beyond the sea, under the protection of his foreign paymasters, perhaps even -- so it was occasionally rumoured -- in some hiding-place in'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='occasionally rumoured -- in some hiding-place in Oceania itself.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Winston's diaphragm was constricted. He could never see the face of Goldstein without a painful mixture of emotions. It was a lean Jewish face, with a great fuzzy aureole of white hair and a small\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='a great fuzzy aureole of white hair and a small goatee beard -- a clever face, and yet somehow inherently despicable, with a kind of senile silliness in the long thin nose, near the end of which a'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='in the long thin nose, near the end of which a pair of spectacles was perched. It resembled the face of a sheep, and the voice, too, had a sheep-like quality. Goldstein was delivering his usual'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='quality. Goldstein was delivering his usual venomous attack upon the doctrines of the Party -- an attack so exaggerated and perverse that a child should have been able to see through it, and yet just'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='have been able to see through it, and yet just plausible enough to fill one with an alarmed feeling that other people, less level-headed than oneself, might be taken in by it. He was abusing Big'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='might be taken in by it. He was abusing Big Brother, he was denouncing the dictatorship of the Party, he was demanding the immediate conclusion of peace with Eurasia, he was advocating freedom of'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='peace with Eurasia, he was advocating freedom of speech, freedom of the Press, freedom of assembly, freedom of thought, he was crying hysterically that the revolution had been betrayed -- and all'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='that the revolution had been betrayed -- and all this in rapid polysyllabic speech which was a sort of parody of the habitual style of the orators of the Party, and even contained Newspeak words:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of the Party, and even contained Newspeak words: more Newspeak words, indeed, than any Party member would normally use in real life. And all the while, lest one should be in any doubt as to the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"while, lest one should be in any doubt as to the reality which Goldstein's specious claptrap covered, behind his head on the telescreen there marched the endless columns of the Eurasian army -- row\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the endless columns of the Eurasian army -- row after row of solid-looking men with expressionless Asiatic faces, who swam up to the surface of the screen and vanished, to be replaced by others'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"the screen and vanished, to be replaced by others exactly similar. The dull rhythmic tramp of the soldiers' boots formed the background to Goldstein's bleating voice.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Before the Hate had proceeded for thirty seconds, uncontrollable exclamations of rage were breaking out from half the people in the room. The self-satisfied sheep-like face on the screen, and the'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='sheep-like face on the screen, and the terrifying power of the Eurasian army behind it, were too much to be borne: besides, the sight or even the thought of Goldstein produced fear and anger'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the thought of Goldstein produced fear and anger automatically. He was an object of hatred more constant than either Eurasia or Eastasia, since when Oceania was at war with one of these Powers it was'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='was at war with one of these Powers it was generally at peace with the other. But what was strange was that although Goldstein was hated and despised by everybody, although every day and a thousand'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='by everybody, although every day and a thousand times a day, on platforms, on the telescreen, in newspapers, in books, his theories were refuted, smashed, ridiculed, held up to the general gaze for'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='ridiculed, held up to the general gaze for the pitiful rubbish that they were in spite of all this, his influence never seemed to grow less. Always there were fresh dupes waiting to be seduced by'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='there were fresh dupes waiting to be seduced by him. A day never passed when spies and saboteurs acting under his directions were not unmasked by the Thought Police. He was the commander of a vast'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Thought Police. He was the commander of a vast shadowy army, an underground network of conspirators dedicated to the overthrow of the State. The Brotherhood, its name was supposed to be. There were'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='its name was supposed to be. There were also whispered stories of a terrible book, a compendium of all the heresies, of which Goldstein was the author and which circulated clandestinely here and'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='and which circulated clandestinely here and there. It was a book without a title. People referred to it, if at all, simply as the book. But one knew of such things only through vague rumours. Neither'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='such things only through vague rumours. Neither the Brotherhood nor the book was a subject that any ordinary Party member would mention if there was a way of avoiding it.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='In its second minute the Hate rose to a frenzy. People were leaping up and down in their places and shouting at the tops of their voices in an effort to drown the maddening bleating voice that came'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='to drown the maddening bleating voice that came from the screen. The little sandy-haired woman had turned bright pink, and her mouth was opening and shutting like that of a landed fish. Even'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"and shutting like that of a landed fish. Even O'Brien's heavy face was flushed. He was sitting very straight in his chair, his powerful chest swelling and quivering as though he were standing up to\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"and quivering as though he were standing up to the assault of a wave. The dark-haired girl behind Winston had begun crying out 'Swine! Swine! Swine!' and suddenly she picked up a heavy Newspeak\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"and suddenly she picked up a heavy Newspeak dictionary and flung it at the screen. It struck Goldstein's nose and bounced off; the voice continued inexorably. In a lucid moment Winston found that he\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='In a lucid moment Winston found that he was shouting with the others and kicking his heel violently against the rung of his chair. The horrible thing about the Two Minutes Hate was not that one was'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='about the Two Minutes Hate was not that one was obliged to act a part, but, on the contrary, that it was impossible to avoid joining in. Within thirty seconds any pretence was always unnecessary. A'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='seconds any pretence was always unnecessary. A hideous ecstasy of fear and vindictiveness, a desire to kill, to torture, to smash faces in with a sledge-hammer, seemed to flow through the whole group'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"seemed to flow through the whole group of people like an electric current, turning one even against one's will into a grimacing, screaming lunatic. And yet the rage that one felt was an abstract,\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"And yet the rage that one felt was an abstract, undirected emotion which could be switched from one object to another like the flame of a blowlamp. Thus, at one moment Winston's hatred was not turned\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"at one moment Winston's hatred was not turned against Goldstein at all, but, on the contrary, against Big Brother, the Party, and the Thought Police; and at such moments his heart went out to the\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='and at such moments his heart went out to the lonely, derided heretic on the screen, sole guardian of truth and sanity in a world of lies. And yet the very next instant he was at one with the people'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='very next instant he was at one with the people about him, and all that was said of Goldstein seemed to him to be true. At those moments his secret loathing of Big Brother changed into adoration, and'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of Big Brother changed into adoration, and Big Brother seemed to tower up, an invincible, fearless protector, standing like a rock against the hordes of Asia, and Goldstein, in spite of his'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='hordes of Asia, and Goldstein, in spite of his isolation, his helplessness, and the doubt that hung about his very existence, seemed like some sinister enchanter, capable by the mere power of his'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='enchanter, capable by the mere power of his voice of wrecking the structure of civilization.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"It was even possible, at moments, to switch one's hatred this way or that by a voluntary act. Suddenly, by the sort of violent effort with which one wrenches one's head away from the pillow in a\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"one wrenches one's head away from the pillow in a nightmare, Winston succeeded in transferring his hatred from the face on the screen to the dark-haired girl behind him. Vivid, beautiful\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the dark-haired girl behind him. Vivid, beautiful hallucinations flashed through his mind. He would flog her to death with a rubber truncheon. He would tie her naked to a stake and shoot her full of'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='tie her naked to a stake and shoot her full of arrows like Saint Sebastian. He would ravish her and cut her throat at the moment of climax. Better than before, moreover, he realized why it was that'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='before, moreover, he realized why it was that he hated her. He hated her because she was young and pretty and sexless, because he wanted to go to bed with her and would never do so, because round her'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='with her and would never do so, because round her sweet supple waist, which seemed to ask you to encircle it with your arm, there was only the odious scarlet sash, aggressive symbol of chastity.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The Hate rose to its climax. The voice of Goldstein had become an actual sheep's bleat, and for an instant the face changed into that of a sheep. Then the sheep-face melted into the figure of a\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Then the sheep-face melted into the figure of a Eurasian soldier who seemed to be advancing, huge and terrible, his sub-machine gun roaring, and seeming to spring out of the surface of the screen, so'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='to spring out of the surface of the screen, so that some of the people in the front row actually flinched backwards in their seats. But in the same moment, drawing a deep sigh of relief from'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"same moment, drawing a deep sigh of relief from everybody, the hostile figure melted into the face of Big Brother, black-haired, black-moustachio'd, full of power and mysterious calm, and so vast\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='full of power and mysterious calm, and so vast that it almost filled up the screen. Nobody heard what Big Brother was saying. It was merely a few words of encouragement, the sort of words that are'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='of encouragement, the sort of words that are uttered in the din of battle, not distinguishable individually but restoring confidence by the fact of being spoken. Then the face of Big Brother faded'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='being spoken. Then the face of Big Brother faded away again, and instead the three slogans of the Party stood out in bold capitals:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='WAR IS PEACE\\n\\nFREEDOM IS SLAVERY\\n\\nIGNORANCE IS STRENGTH'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"But the face of Big Brother seemed to persist for several seconds on the screen, as though the impact that it had made on everyone's eyeballs was too vivid to wear off immediately. The little\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"was too vivid to wear off immediately. The little sandyhaired woman had flung herself forward over the back of the chair in front of her. With a tremulous murmur that sounded like 'My Saviour!' she\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"murmur that sounded like 'My Saviour!' she extended her arms towards the screen. Then she buried her face in her hands. It was apparent that she was uttering a prayer.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"At this moment the entire group of people broke into a deep, slow, rhythmical chant of 'B-B! ...B-B!' -- over and over again, very slowly, with a long pause between the first 'B' and the second-a\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"long pause between the first 'B' and the second-a heavy, murmurous sound, somehow curiously savage, in the background of which one seemed to hear the stamp of naked feet and the throbbing of\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='hear the stamp of naked feet and the throbbing of tom-toms. For perhaps as much as thirty seconds they kept it up. It was a refrain that was often heard in moments of overwhelming emotion. Partly it'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='in moments of overwhelming emotion. Partly it was a sort of hymn to the wisdom and majesty of Big Brother, but still more it was an act of self-hypnosis, a deliberate drowning of consciousness by'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"a deliberate drowning of consciousness by means of rhythmic noise. Winston's entrails seemed to grow cold. In the Two Minutes Hate he could not help sharing in the general delirium, but this\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"help sharing in the general delirium, but this sub-human chanting of 'B-B! ...B-B!' always filled him with horror. Of course he chanted with the rest: it was impossible to do otherwise. To dissemble\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='it was impossible to do otherwise. To dissemble your feelings, to control your face, to do what everyone else was doing, was an instinctive reaction. But there was a space of a couple of seconds'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='But there was a space of a couple of seconds during which the expression of his eyes might conceivably have betrayed him. And it was exactly at this moment that the significant thing happened -- if,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='moment that the significant thing happened -- if, indeed, it did happen.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Momentarily he caught O'Brien's eye. O'Brien had stood up. He had taken off his spectacles and was in the act of resettling them on his nose with his characteristic gesture. But there was a fraction\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"characteristic gesture. But there was a fraction of a second when their eyes met, and for as long as it took to happen Winston knew-yes, he knew!-that O'Brien was thinking the same thing as himself.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"O'Brien was thinking the same thing as himself. An unmistakable message had passed. It was as though their two minds had opened and the thoughts were flowing from one into the other through their\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"flowing from one into the other through their eyes. 'I am with you,' O'Brien seemed to be saying to him. 'I know precisely what you are feeling. I know all about your contempt, your hatred, your\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"I know all about your contempt, your hatred, your disgust. But don't worry, I am on your side!' And then the flash of intelligence was gone, and O'Brien's face was as inscrutable as everybody else's.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='That was all, and he was already uncertain whether it had happened. Such incidents never had any sequel. All that they did was to keep alive in him the belief, or hope, that others besides himself'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the belief, or hope, that others besides himself were the enemies of the Party. Perhaps the rumours of vast underground conspiracies were true after all -- perhaps the Brotherhood really existed! It'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='all -- perhaps the Brotherhood really existed! It was impossible, in spite of the endless arrests and confessions and executions, to be sure that the Brotherhood was not simply a myth. Some days he'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Brotherhood was not simply a myth. Some days he believed in it, some days not. There was no evidence, only fleeting glimpses that might mean anything or nothing: snatches of overheard conversation,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='or nothing: snatches of overheard conversation, faint scribbles on lavatory walls -- once, even, when two strangers met, a small movement of the hand which had looked as though it might be a signal'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"which had looked as though it might be a signal of recognition. It was all guesswork: very likely he had imagined everything. He had gone back to his cubicle without looking at O'Brien again. The\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"his cubicle without looking at O'Brien again. The idea of following up their momentary contact hardly crossed his mind. It would have been inconceivably dangerous even if he had known how to set\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='dangerous even if he had known how to set about doing it. For a second, two seconds, they had exchanged an equivocal glance, and that was the end of the story. But even that was a memorable event, in'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='story. But even that was a memorable event, in the locked loneliness in which one had to live.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston roused himself and sat up straighter. He let out a belch. The gin was rising from his stomach.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='His eyes re-focused on the page. He discovered that while he sat helplessly musing he had also been writing, as though by automatic action. And it was no longer the same cramped, awkward handwriting'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='no longer the same cramped, awkward handwriting as before. His pen had slid voluptuously over the smooth paper, printing in large neat capitals -'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='DOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nDOWN WITH BIG BROTHER\\n\\nover and over again, filling half a page.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='He could not help feeling a twinge of panic. It was absurd, since the writing of those particular words was not more dangerous than the initial act of opening the diary, but for a moment he was'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='act of opening the diary, but for a moment he was tempted to tear out the spoiled pages and abandon the enterprise altogether.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='He did not do so, however, because he knew that it was useless. Whether he wrote DOWN WITH BIG BROTHER, or whether he refrained from writing it, made no difference. Whether he went on with the diary,'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='no difference. Whether he went on with the diary, or whether he did not go on with it, made no difference. The Thought Police would get him just the same. He had committed -- would still have'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='the same. He had committed -- would still have committed, even if he had never set pen to paper -- the essential crime that contained all others in itself. Thoughtcrime, they called it. Thoughtcrime'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Thoughtcrime, they called it. Thoughtcrime was not a thing that could be concealed for ever. You might dodge successfully for a while, even for years, but sooner or later they were bound to get you.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It was always at night -- the arrests invariably happened at night. The sudden jerk out of sleep, the rough hand shaking your shoulder, the lights glaring in your eyes, the ring of hard faces round'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='in your eyes, the ring of hard faces round the bed. In the vast majority of cases there was no trial, no report of the arrest. People simply disappeared, always during the night. Your name was'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='always during the night. Your name was removed from the registers, every record of everything you had ever done was wiped out, your one-time existence was denied and then forgotten. You were'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='existence was denied and then forgotten. You were abolished, annihilated: vaporized was the usual word.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For a moment he was seized by a kind of hysteria. He began writing in a hurried untidy scrawl:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"theyll shoot me i don't care theyll shoot me in the back of the neck i dont care down with big brother they always shoot you in the back of the neck i dont care down with big brother --\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='He sat back in his chair, slightly ashamed of himself, and laid down the pen. The next moment he started violently. There was a knocking at the door.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Already! He sat as still as a mouse, in the futile hope that whoever it was might go away after a single attempt. But no, the knocking was repeated. The worst thing of all would be to delay. His'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The worst thing of all would be to delay. His heart was thumping like a drum, but his face, from long habit, was probably expressionless. He got up and moved heavily towards the door.')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader #어떤 확장자의 파일이든 불러올 수 있음\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #generic. 아주 일반적으로 사용하는 splitter. 문장이나 문단 끝마다 split\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, #chunk_size: 더 작은 단위의 document로 잘라냼. but 문장도 다 잘라버림\n",
    "    chunk_overlap=50 #문장을 자르지 않기 위해 앞의 토큰 몇 개를 뒤에 붙임\n",
    ") \n",
    "loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "# loader.load() #전부 load. 하나의 document가 들어있는 리스트가 반환\n",
    "loader.load_and_split(text_splitter=splitter) #chunk_size를 주지 않으면 11개의 document가 들어있는 리스트가 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a2a908ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 963, which is longer than the specified 600\n",
      "Created a chunk of size 774, which is longer than the specified 600\n",
      "Created a chunk of size 954, which is longer than the specified 600\n",
      "Created a chunk of size 922, which is longer than the specified 600\n",
      "Created a chunk of size 1168, which is longer than the specified 600\n",
      "Created a chunk of size 821, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 745, which is longer than the specified 600\n",
      "Created a chunk of size 735, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 991, which is longer than the specified 600\n",
      "Created a chunk of size 990, which is longer than the specified 600\n",
      "Created a chunk of size 1741, which is longer than the specified 600\n",
      "Created a chunk of size 2001, which is longer than the specified 600\n",
      "Created a chunk of size 1900, which is longer than the specified 600\n",
      "Created a chunk of size 1130, which is longer than the specified 600\n",
      "Created a chunk of size 1799, which is longer than the specified 600\n",
      "Created a chunk of size 1690, which is longer than the specified 600\n",
      "Created a chunk of size 2364, which is longer than the specified 600\n",
      "Created a chunk of size 930, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1260, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 1293, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './files/chapter_one.txt'}, page_content='Part One\\n1\\nIt was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Inside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Outside, even through the shut window-pane, the world looked cold. Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere. The blackmoustachio'd face gazed down from every commanding corner. There was one on the house-front immediately opposite. BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked deep into Winston's own. Down at streetlevel another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering the single word INGSOC. In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away again with a curving flight. It was the police patrol, snooping into people's windows. The patrols did not matter, however. Only the Thought Police mattered.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Behind Winston's back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan. The telescreen received and transmitted simultaneously. Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard. There was of course no way of knowing whether you were being watched at any given moment. How often, or on what system, the Thought Police plugged in on any individual wire was guesswork. It was even conceivable that they watched everybody all the time. But at any rate they could plug in your wire whenever they wanted to. You had to live -- did live, from habit that became instinct -- in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston kept his back turned to the telescreen. It was safer, though, as he well knew, even a back can be revealing. A kilometre away the Ministry of Truth, his place of work, towered vast and white above the grimy landscape. This, he thought with a sort of vague distaste -- this was London, chief city of Airstrip One, itself the third most populous of the provinces of Oceania. He tried to squeeze out some childhood memory that should tell him whether London had always been quite like this. Were there always these vistas of rotting nineteenth-century houses, their sides shored up with baulks of timber, their windows patched with cardboard and their roofs with corrugated iron, their crazy garden walls sagging in all directions? And the bombed sites where the plaster dust swirled in the air and the willow-herb straggled over the heaps of rubble; and the places where the bombs had cleared a larger patch and there had sprung up sordid colonies of wooden dwellings like chicken-houses? But it was no use, he could not remember: nothing remained of his childhood except a series of bright-lit tableaux occurring against no background and mostly unintelligible.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The Ministry of Truth -- Minitrue, in Newspeak -- was startlingly different from any other object in sight. It was an enormous pyramidal structure of glittering white concrete, soaring up, terrace after terrace, 300 metres into the air. From where Winston stood it was just possible to read, picked out on its white face in elegant lettering, the three slogans of the Party:\\nWAR IS PEACE\\nFREEDOM IS SLAVERY\\nIGNORANCE IS STRENGTH'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The Ministry of Truth contained, it was said, three thousand rooms above ground level, and corresponding ramifications below. Scattered about London there were just three other buildings of similar appearance and size. So completely did they dwarf the surrounding architecture that from the roof of Victory Mansions you could see all four of them simultaneously. They were the homes of the four Ministries between which the entire apparatus of government was divided. The Ministry of Truth, which concerned itself with news, entertainment, education, and the fine arts. The Ministry of Peace, which concerned itself with war. The Ministry of Love, which maintained law and order. And the Ministry of Plenty, which was responsible for economic affairs. Their names, in Newspeak: Minitrue, Minipax, Miniluv, and Miniplenty.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The Ministry of Love was the really frightening one. There were no windows in it at all. Winston had never been inside the Ministry of Love, nor within half a kilometre of it. It was a place impossible to enter except on official business, and then only by penetrating through a maze of barbed-wire entanglements, steel doors, and hidden machine-gun nests. Even the streets leading up to its outer barriers were roamed by gorilla-faced guards in black uniforms, armed with jointed truncheons.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Winston turned round abruptly. He had set his features into the expression of quiet optimism which it was advisable to wear when facing the telescreen. He crossed the room into the tiny kitchen. By leaving the Ministry at this time of day he had sacrificed his lunch in the canteen, and he was aware that there was no food in the kitchen except a hunk of dark-coloured bread which had got to be saved for tomorrow's breakfast. He took down from the shelf a bottle of colourless liquid with a plain white label marked VICTORY GIN. It gave off a sickly, oily smell, as of Chinese ricespirit. Winston poured out nearly a teacupful, nerved himself for a shock, and gulped it down like a dose of medicine.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Instantly his face turned scarlet and the water ran out of his eyes. The stuff was like nitric acid, and moreover, in swallowing it one had the sensation of being hit on the back of the head with a rubber club. The next moment, however, the burning in his belly died down and the world began to look more cheerful. He took a cigarette from a crumpled packet marked VICTORY CIGARETTES and incautiously held it upright, whereupon the tobacco fell out on to the floor. With the next he was more successful. He went back to the living-room and sat down at a small table that stood to the left of the telescreen. From the table drawer he took out a penholder, a bottle of ink, and a thick, quarto-sized blank book with a red back and a marbled cover.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For some reason the telescreen in the living-room was in an unusual position. Instead of being placed, as was normal, in the end wall, where it could command the whole room, it was in the longer wall, opposite the window. To one side of it there was a shallow alcove in which Winston was now sitting, and which, when the flats were built, had probably been intended to hold bookshelves. By sitting in the alcove, and keeping well back, Winston was able to remain outside the range of the telescreen, so far as sight went. He could be heard, of course, but so long as he stayed in his present position he could not be seen. It was partly the unusual geography of the room that had suggested to him the thing that he was now about to do.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"But it had also been suggested by the book that he had just taken out of the drawer. It was a peculiarly beautiful book. Its smooth creamy paper, a little yellowed by age, was of a kind that had not been manufactured for at least forty years past. He could guess, however, that the book was much older than that. He had seen it lying in the window of a frowsy little junk-shop in a slummy quarter of the town (just what quarter he did not now remember) and had been stricken immediately by an overwhelming desire to possess it. Party members were supposed not to go into ordinary shops ('dealing on the free market', it was called), but the rule was not strictly kept, because there were various things, such as shoelaces and razor blades, which it was impossible to get hold of in any other way. He had given a quick glance up and down the street and then had slipped inside and bought the book for two dollars fifty. At the time he was not conscious of wanting it for any particular purpose. He had carried it guiltily home in his briefcase. Even with nothing written in it, it was a compromising possession.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='The thing that he was about to do was to open a diary. This was not illegal (nothing was illegal, since there were no longer any laws), but if detected it was reasonably certain that it would be punished by death, or at least by twenty-five years in a forced-labour camp. Winston fitted a nib into the penholder and sucked it to get the grease off. The pen was an archaic instrument, seldom used even for signatures, and he had procured one, furtively and with some difficulty, simply because of a feeling that the beautiful creamy paper deserved to be written on with a real nib instead of being scratched with an ink-pencil. Actually he was not used to writing by hand. Apart from very short notes, it was usual to dictate everything into the speakwrite which was of course impossible for his present purpose. He dipped the pen into the ink and then faltered for just a second. A tremor had gone through his bowels. To mark the paper was the decisive act. In small clumsy letters he wrote:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='April 4th, 1984.\\nHe sat back. A sense of complete helplessness had descended upon him. To begin with, he did not know with any certainty that this was 1984. It must be round about that date, since he was fairly sure that his age was thirty-nine, and he believed that he had been born in 1944 or 1945; but it was never possible nowadays to pin down any date within a year or two.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For whom, it suddenly occurred to him to wonder, was he writing this diary? For the future, for the unborn. His mind hovered for a moment round the doubtful date on the page, and then fetched up with a bump against the Newspeak word doublethink. For the first time the magnitude of what he had undertaken came home to him. How could you communicate with the future? It was of its nature impossible. Either the future would resemble the present, in which case it would not listen to him: or it would be different from it, and his predicament would be meaningless.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='For some time he sat gazing stupidly at the paper. The telescreen had changed over to strident military music. It was curious that he seemed not merely to have lost the power of expressing himself, but even to have forgotten what it was that he had originally intended to say. For weeks past he had been making ready for this moment, and it had never crossed his mind that anything would be needed except courage. The actual writing would be easy. All he had to do was to transfer to paper the interminable restless monologue that had been running inside his head, literally for years. At this moment, however, even the monologue had dried up. Moreover his varicose ulcer had begun itching unbearably. He dared not scratch it, because if he did so it always became inflamed. The seconds were ticking by. He was conscious of nothing except the blankness of the page in front of him, the itching of the skin above his ankle, the blaring of the music, and a slight booziness caused by the gin.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Suddenly he began writing in sheer panic, only imperfectly aware of what he was setting down. His small but childish handwriting straggled up and down the page, shedding first its capital letters and finally even its full stops:'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"April 4th, 1984. Last night to the flicks. All war films. One very good one of a ship full of refugees being bombed somewhere in the Mediterranean. Audience much amused by shots of a great huge fat man trying to swim away with a helicopter after him, first you saw him wallowing along in the water like a porpoise, then you saw him through the helicopters gunsights, then he was full of holes and the sea round him turned pink and he sank as suddenly as though the holes had let in the water, audience shouting with laughter when he sank. then you saw a lifeboat full of children with a helicopter hovering over it. there was a middle-aged woman might have been a jewess sitting up in the bow with a little boy about three years old in her arms. little boy screaming with fright and hiding his head between her breasts as if he was trying to burrow right into her and the woman putting her arms round him and comforting him although she was blue with fright herself, all the time covering him up as much as possible as if she thought her arms could keep the bullets off him. then the helicopter planted a 20 kilo bomb in among them terrific flash and the boat went all to matchwood. then there was a wonderful shot of a child's arm going up up up right up into the air a helicopter with a camera in its nose must have followed it up and there was a lot of applause from the party seats but a woman down in the prole part of the house suddenly started kicking up a fuss and shouting they didnt oughter of showed it not in front of kids they didnt it aint right not in front of kids it aint until the police turned her turned her out i dont suppose anything happened to her nobody cares what the proles say typical prole reaction they never --\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston stopped writing, partly because he was suffering from cramp. He did not know what had made him pour out this stream of rubbish. But the curious thing was that while he was doing so a totally different memory had clarified itself in his mind, to the point where he almost felt equal to writing it down. It was, he now realized, because of this other incident that he had suddenly decided to come home and begin the diary today.\\nIt had happened that morning at the Ministry, if anything so nebulous could be said to happen.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It was nearly eleven hundred, and in the Records Department, where Winston worked, they were dragging the chairs out of the cubicles and grouping them in the centre of the hall opposite the big telescreen, in preparation for the Two Minutes Hate. Winston was just taking his place in one of the middle rows when two people whom he knew by sight, but had never spoken to, came unexpectedly into the room. One of them was a girl whom he often passed in the corridors. He did not know her name, but he knew that she worked in the Fiction Department. Presumably -- since he had sometimes seen her with oily hands and carrying a spanner she had some mechanical job on one of the novel-writing machines. She was a bold-looking girl, of about twenty-seven, with thick hair, a freckled face, and swift, athletic movements. A narrow scarlet sash, emblem of the Junior Anti-Sex League, was wound several times round the waist of her overalls, just tightly enough to bring out the shapeliness of her hips. Winston had disliked her from the very first moment of seeing her. He knew the reason. It was because of the atmosphere of hockey-fields and cold baths and community hikes and general clean-mindedness which she managed to carry about with her. He disliked nearly all women, and especially the young and pretty ones. It was always the women, and above all the young ones, who were the most bigoted adherents of the Party, the swallowers of slogans, the amateur spies and nosers-out of unorthodoxy. But this particular girl gave him the impression of being more dangerous than most. Once when they passed in the corridor she gave him a quick sidelong glance which seemed to pierce right into him and for a moment had filled him with black terror. The idea had even crossed his mind that she might be an agent of the Thought Police. That, it was true, was very unlikely. Still, he continued to feel a peculiar uneasiness, which had fear mixed up in it as well as hostility, whenever she was anywhere near him.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The other person was a man named O'Brien, a member of the Inner Party and holder of some post so important and remote that Winston had only a dim idea of its nature. A momentary hush passed over the group of people round the chairs as they saw the black overalls of an Inner Party member approaching. O'Brien was a large, burly man with a thick neck and a coarse, humorous, brutal face. In spite of his formidable appearance he had a certain charm of manner. He had a trick of resettling his spectacles on his nose which was curiously disarming -- in some indefinable way, curiously civilized. It was a gesture which, if anyone had still thought in such terms, might have recalled an eighteenth-century nobleman offering his snuffbox. Winston had seen O'Brien perhaps a dozen times in almost as many years. He felt deeply drawn to him, and not solely because he was intrigued by the contrast between O'Brien's urbane manner and his prize-fighter's physique. Much more it was because of a secretly held belief -- or perhaps not even a belief, merely a hope -- that O'Brien's political orthodoxy was not perfect. Something in his face suggested it irresistibly. And again, perhaps it was not even unorthodoxy that was written in his face, but simply intelligence. But at any rate he had the appearance of being a person that you could talk to if somehow you could cheat the telescreen and get him alone. Winston had never made the smallest effort to verify this guess: indeed, there was no way of doing so. At this moment O'Brien glanced at his wrist-watch, saw that it was nearly eleven hundred, and evidently decided to stay in the Records Department until the Two Minutes Hate was over. He took a chair in the same row as Winston, a couple of places away. A small, sandy-haired woman who worked in the next cubicle to Winston was between them. The girl with dark hair was sitting immediately behind.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The next moment a hideous, grinding speech, as of some monstrous machine running without oil, burst from the big telescreen at the end of the room. It was a noise that set one's teeth on edge and bristled the hair at the back of one's neck. The Hate had started.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"As usual, the face of Emmanuel Goldstein, the Enemy of the People, had flashed on to the screen. There were hisses here and there among the audience. The little sandy-haired woman gave a squeak of mingled fear and disgust. Goldstein was the renegade and backslider who once, long ago (how long ago, nobody quite remembered), had been one of the leading figures of the Party, almost on a level with Big Brother himself, and then had engaged in counter-revolutionary activities, had been condemned to death, and had mysteriously escaped and disappeared. The programmes of the Two Minutes Hate varied from day to day, but there was none in which Goldstein was not the principal figure. He was the primal traitor, the earliest defiler of the Party's purity. All subsequent crimes against the Party, all treacheries, acts of sabotage, heresies, deviations, sprang directly out of his teaching. Somewhere or other he was still alive and hatching his conspiracies: perhaps somewhere beyond the sea, under the protection of his foreign paymasters, perhaps even -- so it was occasionally rumoured -- in some hiding-place in Oceania itself.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Winston's diaphragm was constricted. He could never see the face of Goldstein without a painful mixture of emotions. It was a lean Jewish face, with a great fuzzy aureole of white hair and a small goatee beard -- a clever face, and yet somehow inherently despicable, with a kind of senile silliness in the long thin nose, near the end of which a pair of spectacles was perched. It resembled the face of a sheep, and the voice, too, had a sheep-like quality. Goldstein was delivering his usual venomous attack upon the doctrines of the Party -- an attack so exaggerated and perverse that a child should have been able to see through it, and yet just plausible enough to fill one with an alarmed feeling that other people, less level-headed than oneself, might be taken in by it. He was abusing Big Brother, he was denouncing the dictatorship of the Party, he was demanding the immediate conclusion of peace with Eurasia, he was advocating freedom of speech, freedom of the Press, freedom of assembly, freedom of thought, he was crying hysterically that the revolution had been betrayed -- and all this in rapid polysyllabic speech which was a sort of parody of the habitual style of the orators of the Party, and even contained Newspeak words: more Newspeak words, indeed, than any Party member would normally use in real life. And all the while, lest one should be in any doubt as to the reality which Goldstein's specious claptrap covered, behind his head on the telescreen there marched the endless columns of the Eurasian army -- row after row of solid-looking men with expressionless Asiatic faces, who swam up to the surface of the screen and vanished, to be replaced by others exactly similar. The dull rhythmic tramp of the soldiers' boots formed the background to Goldstein's bleating voice.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Before the Hate had proceeded for thirty seconds, uncontrollable exclamations of rage were breaking out from half the people in the room. The self-satisfied sheep-like face on the screen, and the terrifying power of the Eurasian army behind it, were too much to be borne: besides, the sight or even the thought of Goldstein produced fear and anger automatically. He was an object of hatred more constant than either Eurasia or Eastasia, since when Oceania was at war with one of these Powers it was generally at peace with the other. But what was strange was that although Goldstein was hated and despised by everybody, although every day and a thousand times a day, on platforms, on the telescreen, in newspapers, in books, his theories were refuted, smashed, ridiculed, held up to the general gaze for the pitiful rubbish that they were in spite of all this, his influence never seemed to grow less. Always there were fresh dupes waiting to be seduced by him. A day never passed when spies and saboteurs acting under his directions were not unmasked by the Thought Police. He was the commander of a vast shadowy army, an underground network of conspirators dedicated to the overthrow of the State. The Brotherhood, its name was supposed to be. There were also whispered stories of a terrible book, a compendium of all the heresies, of which Goldstein was the author and which circulated clandestinely here and there. It was a book without a title. People referred to it, if at all, simply as the book. But one knew of such things only through vague rumours. Neither the Brotherhood nor the book was a subject that any ordinary Party member would mention if there was a way of avoiding it.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"In its second minute the Hate rose to a frenzy. People were leaping up and down in their places and shouting at the tops of their voices in an effort to drown the maddening bleating voice that came from the screen. The little sandy-haired woman had turned bright pink, and her mouth was opening and shutting like that of a landed fish. Even O'Brien's heavy face was flushed. He was sitting very straight in his chair, his powerful chest swelling and quivering as though he were standing up to the assault of a wave. The dark-haired girl behind Winston had begun crying out 'Swine! Swine! Swine!' and suddenly she picked up a heavy Newspeak dictionary and flung it at the screen. It struck Goldstein's nose and bounced off; the voice continued inexorably. In a lucid moment Winston found that he was shouting with the others and kicking his heel violently against the rung of his chair. The horrible thing about the Two Minutes Hate was not that one was obliged to act a part, but, on the contrary, that it was impossible to avoid joining in. Within thirty seconds any pretence was always unnecessary. A hideous ecstasy of fear and vindictiveness, a desire to kill, to torture, to smash faces in with a sledge-hammer, seemed to flow through the whole group of people like an electric current, turning one even against one's will into a grimacing, screaming lunatic. And yet the rage that one felt was an abstract, undirected emotion which could be switched from one object to another like the flame of a blowlamp. Thus, at one moment Winston's hatred was not turned against Goldstein at all, but, on the contrary, against Big Brother, the Party, and the Thought Police; and at such moments his heart went out to the lonely, derided heretic on the screen, sole guardian of truth and sanity in a world of lies. And yet the very next instant he was at one with the people about him, and all that was said of Goldstein seemed to him to be true. At those moments his secret loathing of Big Brother changed into adoration, and Big Brother seemed to tower up, an invincible, fearless protector, standing like a rock against the hordes of Asia, and Goldstein, in spite of his isolation, his helplessness, and the doubt that hung about his very existence, seemed like some sinister enchanter, capable by the mere power of his voice of wrecking the structure of civilization.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"It was even possible, at moments, to switch one's hatred this way or that by a voluntary act. Suddenly, by the sort of violent effort with which one wrenches one's head away from the pillow in a nightmare, Winston succeeded in transferring his hatred from the face on the screen to the dark-haired girl behind him. Vivid, beautiful hallucinations flashed through his mind. He would flog her to death with a rubber truncheon. He would tie her naked to a stake and shoot her full of arrows like Saint Sebastian. He would ravish her and cut her throat at the moment of climax. Better than before, moreover, he realized why it was that he hated her. He hated her because she was young and pretty and sexless, because he wanted to go to bed with her and would never do so, because round her sweet supple waist, which seemed to ask you to encircle it with your arm, there was only the odious scarlet sash, aggressive symbol of chastity.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"The Hate rose to its climax. The voice of Goldstein had become an actual sheep's bleat, and for an instant the face changed into that of a sheep. Then the sheep-face melted into the figure of a Eurasian soldier who seemed to be advancing, huge and terrible, his sub-machine gun roaring, and seeming to spring out of the surface of the screen, so that some of the people in the front row actually flinched backwards in their seats. But in the same moment, drawing a deep sigh of relief from everybody, the hostile figure melted into the face of Big Brother, black-haired, black-moustachio'd, full of power and mysterious calm, and so vast that it almost filled up the screen. Nobody heard what Big Brother was saying. It was merely a few words of encouragement, the sort of words that are uttered in the din of battle, not distinguishable individually but restoring confidence by the fact of being spoken. Then the face of Big Brother faded away again, and instead the three slogans of the Party stood out in bold capitals:\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"WAR IS PEACE\\nFREEDOM IS SLAVERY\\nIGNORANCE IS STRENGTH\\nBut the face of Big Brother seemed to persist for several seconds on the screen, as though the impact that it had made on everyone's eyeballs was too vivid to wear off immediately. The little sandyhaired woman had flung herself forward over the back of the chair in front of her. With a tremulous murmur that sounded like 'My Saviour!' she extended her arms towards the screen. Then she buried her face in her hands. It was apparent that she was uttering a prayer.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"At this moment the entire group of people broke into a deep, slow, rhythmical chant of 'B-B! ...B-B!' -- over and over again, very slowly, with a long pause between the first 'B' and the second-a heavy, murmurous sound, somehow curiously savage, in the background of which one seemed to hear the stamp of naked feet and the throbbing of tom-toms. For perhaps as much as thirty seconds they kept it up. It was a refrain that was often heard in moments of overwhelming emotion. Partly it was a sort of hymn to the wisdom and majesty of Big Brother, but still more it was an act of self-hypnosis, a deliberate drowning of consciousness by means of rhythmic noise. Winston's entrails seemed to grow cold. In the Two Minutes Hate he could not help sharing in the general delirium, but this sub-human chanting of 'B-B! ...B-B!' always filled him with horror. Of course he chanted with the rest: it was impossible to do otherwise. To dissemble your feelings, to control your face, to do what everyone else was doing, was an instinctive reaction. But there was a space of a couple of seconds during which the expression of his eyes might conceivably have betrayed him. And it was exactly at this moment that the significant thing happened -- if, indeed, it did happen.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"Momentarily he caught O'Brien's eye. O'Brien had stood up. He had taken off his spectacles and was in the act of resettling them on his nose with his characteristic gesture. But there was a fraction of a second when their eyes met, and for as long as it took to happen Winston knew-yes, he knew!-that O'Brien was thinking the same thing as himself. An unmistakable message had passed. It was as though their two minds had opened and the thoughts were flowing from one into the other through their eyes. 'I am with you,' O'Brien seemed to be saying to him. 'I know precisely what you are feeling. I know all about your contempt, your hatred, your disgust. But don't worry, I am on your side!' And then the flash of intelligence was gone, and O'Brien's face was as inscrutable as everybody else's.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"That was all, and he was already uncertain whether it had happened. Such incidents never had any sequel. All that they did was to keep alive in him the belief, or hope, that others besides himself were the enemies of the Party. Perhaps the rumours of vast underground conspiracies were true after all -- perhaps the Brotherhood really existed! It was impossible, in spite of the endless arrests and confessions and executions, to be sure that the Brotherhood was not simply a myth. Some days he believed in it, some days not. There was no evidence, only fleeting glimpses that might mean anything or nothing: snatches of overheard conversation, faint scribbles on lavatory walls -- once, even, when two strangers met, a small movement of the hand which had looked as though it might be a signal of recognition. It was all guesswork: very likely he had imagined everything. He had gone back to his cubicle without looking at O'Brien again. The idea of following up their momentary contact hardly crossed his mind. It would have been inconceivably dangerous even if he had known how to set about doing it. For a second, two seconds, they had exchanged an equivocal glance, and that was the end of the story. But even that was a memorable event, in the locked loneliness in which one had to live.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Winston roused himself and sat up straighter. He let out a belch. The gin was rising from his stomach.\\nHis eyes re-focused on the page. He discovered that while he sat helplessly musing he had also been writing, as though by automatic action. And it was no longer the same cramped, awkward handwriting as before. His pen had slid voluptuously over the smooth paper, printing in large neat capitals -\\nDOWN WITH BIG BROTHER\\nDOWN WITH BIG BROTHER\\nDOWN WITH BIG BROTHER\\nDOWN WITH BIG BROTHER\\nDOWN WITH BIG BROTHER\\nover and over again, filling half a page.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='DOWN WITH BIG BROTHER\\nDOWN WITH BIG BROTHER\\nover and over again, filling half a page.\\nHe could not help feeling a twinge of panic. It was absurd, since the writing of those particular words was not more dangerous than the initial act of opening the diary, but for a moment he was tempted to tear out the spoiled pages and abandon the enterprise altogether.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='He did not do so, however, because he knew that it was useless. Whether he wrote DOWN WITH BIG BROTHER, or whether he refrained from writing it, made no difference. Whether he went on with the diary, or whether he did not go on with it, made no difference. The Thought Police would get him just the same. He had committed -- would still have committed, even if he had never set pen to paper -- the essential crime that contained all others in itself. Thoughtcrime, they called it. Thoughtcrime was not a thing that could be concealed for ever. You might dodge successfully for a while, even for years, but sooner or later they were bound to get you.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='It was always at night -- the arrests invariably happened at night. The sudden jerk out of sleep, the rough hand shaking your shoulder, the lights glaring in your eyes, the ring of hard faces round the bed. In the vast majority of cases there was no trial, no report of the arrest. People simply disappeared, always during the night. Your name was removed from the registers, every record of everything you had ever done was wiped out, your one-time existence was denied and then forgotten. You were abolished, annihilated: vaporized was the usual word.'),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content=\"For a moment he was seized by a kind of hysteria. He began writing in a hurried untidy scrawl:\\ntheyll shoot me i don't care theyll shoot me in the back of the neck i dont care down with big brother they always shoot you in the back of the neck i dont care down with big brother --\\nHe sat back in his chair, slightly ashamed of himself, and laid down the pen. The next moment he started violently. There was a knocking at the door.\"),\n",
       " Document(metadata={'source': './files/chapter_one.txt'}, page_content='Already! He sat as still as a mouse, in the futile hope that whoever it was might go away after a single attempt. But no, the knocking was repeated. The worst thing of all would be to delay. His heart was thumping like a drum, but his face, from long habit, was probably expressionless. He got up and moved heavily towards the door.')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter #특정 separator를 기준으로 split\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600, #separator 유무에 따라 더 커질수도, 작아질 수도 있음. chunk의 사이즈를 계산하는 데에는 파이썬 표준 라이브러리의 len 함수 사용\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter) #11개의 document가 들어있는 리스트가 반환됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb52fdc",
   "metadata": {},
   "source": [
    "### 6.2 Tiktoken\n",
    "* chunk의 사이즈를 계산하는 데에는 파이썬 표준 라이브러리의 len 함수 사용. length_funtion arg를 사용해서 token을 세는 것으로 변경 가능\n",
    "* openai의 tokenizer(tiktoken) : https://platform.openai.com/tokenizer\n",
    "* The models learn to understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder( #이제 우리가 size를 계산하는 법이 OpenAI가 size를 계산하는 법과 같음\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = TextLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6d16d",
   "metadata": {},
   "source": [
    "### 6.3 Vectors\n",
    "* vector: 단어를 숫자로 변환. 단어 사이의 연산이 가능해짐\n",
    "* https://turbomaze.github.io/word2vecjson/ : 비슷한 벡터를 가진 단어들을 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07115b8e",
   "metadata": {},
   "source": [
    "### 6.4 Vector Store\n",
    "* Chroma: 오픈소스 vector store. 클라우드가 아니라 우리 컴퓨터에서 실행됨. 우린 나중에 pinecone과 같은 클라우드 벡터 스토어를 쓸거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "53aa9df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "4\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings #OpenAI가 개발, 랭체인이 지원\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "hi_vector = embedder.embed_query(\"Hi\") #단어 하나를 embed\n",
    "print(len(hi_vector)) #1536 dimension\n",
    "\n",
    "vector = embedder.embed_documents([ #document를 embed. 우리는 document를 임베딩할거임\n",
    "    \"소리에 놀라지 않는 사자와 같이\",\n",
    "    \"그물에 걸리지 않는 바람과 같이\",\n",
    "    \"흙탕물에 더럽혀지지 않는 연꽃과 같이\",\n",
    "    \"무소의 뿔처럼 혼자서 가라\"\n",
    "])\n",
    "print(len(vector)) #4개\n",
    "print(len(vector[0])) #각각의 벡터는 1536 dimension을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b17771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './files/chapter_one.txt'}, page_content='Part One\\n1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\nInside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.'), Document(metadata={'source': './files/chapter_one.txt'}, page_content='Part One\\n1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\nInside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.'), Document(metadata={'source': './files/chapter_one.txt'}, page_content='Part One\\n1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\nInside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.'), Document(metadata={'source': './files/chapter_one.txt'}, page_content='Part One\\n1 It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\nThe hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\nInside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings #model: str = \"text-embedding-ada-002\", 저렴함\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\") #여기에 캐시가 생성될거임\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) #vector 생성. 이 라인을 실행하면 요금을 지불해야 함 / embedding을 캐시한 이후로는 캐시된 embedding을 불러올거임\n",
    "\n",
    "results = vectorstore.similarity_search(\"Where does Winston live?\") #결과로 document 4개를 반환함. query와 비슷한 문서. 이 문서랑 내 질문을 같이 모델에 전달해주면 대답해줄거임.\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bd3e4",
   "metadata": {},
   "source": [
    "### 6.5 Langsmith\n",
    "* 우리의 체인이 뭘 하고 있는지 시각적으로 볼 수 있음\n",
    "* https://smith.langchain.com/\n",
    "* .env파일 안에 API_KEY 넣어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f5366",
   "metadata": {},
   "source": [
    "### 6.6 RetrievalQA\n",
    "* 많은 off-the-shelf 체인이 있음. 그러나 현재 권장되는 방식은 LCEL로 내 체인을 만드는 것\n",
    "* <b>RetrievalQA</b>: 랭체인에서 RAG 작업을 쉽게 해주는 고수준 래퍼 클래스. \n",
    "    - retriever: 관련 문서를 가져옴\n",
    "    - chain_type: 문서들을 어떤 방식으로 LLM에 전달할지 결정\n",
    "    - llm: 언어모델\n",
    "* chain types\n",
    "    - <b>stuff</b>: 모든 문서를 한 번에 LLM에 넣고 한 번에 답변\n",
    "    - <b>map_reduce</b>: 각 문서에서 부분 답변 생성(map) -> 종합해서 최종 답변 생성(reduce)\n",
    "    - <b>refine</b>: 첫 문서로 초안 작성 -> 이후 문서로 점점 개선(refine)\n",
    "    - <b>map_rerank</b>: 각 문서에서 답변 생성 -> 가장 좋은 답 하나를 선택. 부분 정보가 무시될 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "459bbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "/Users/boyoon/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain/chains/llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  \"\"\"Call apply and then parse the results.\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Winston lives in Victory Mansions, a building with a hallway that smells of boiled cabbage and old rag mats. The flat is seven flights up, and the building has a poster with an enormous face that says \"BIG BROTHER IS WATCHING YOU.\" Inside the flat, there is a telescreen that cannot be shut off completely.'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#off-the-shelf체인\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "#랭스미스와의 연동을 위해 필요 (2024년 말 업데이트 이후 자동 트레이싱이 비활성화 됨. 수동으로 callbacks를 줘야 함)\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import Chroma # Chroma랑 FAISS는 비슷. 니코쌤 경험으로는 FAISS가 나을 때가 있다고 함. vectorstore를 바꾸려면 cache도 삭제해야 함.\n",
    "\n",
    "#langsmith에서 tracing이 제대로 되도록\n",
    "client = Client()\n",
    "tracer = LangChainTracer(client=client)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, callbacks=[tracer])\n",
    "\n",
    "#document를 load후 split\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "#embedding(&caching)을 통해 vectorstore로 변환\n",
    "cache_dir = LocalFileStore(\"./.cache/\") #여기에 캐시가 생성될거임\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) #vector 생성. 이 라인을 실행하면 요금을 지불해야 함 / embedding을 캐시한 이후로는 캐시된 embedding을 불러올거임\n",
    "\n",
    "#vectorstore 테스트\n",
    "results = vectorstore.similarity_search(\"Where does Winston live?\") #결과로 document 4개를 반환. 질문을 임베딩해야 하기 때문에 API 호출이 1회 일어남.\n",
    "\n",
    "#RetrievalQA 테스트 (off-the-shelf chains). LangSmith를 통해 체인 내부에서 어떤 일이 일어나는지 확인 가능\n",
    "chain = RetrievalQA.from_chain_type( #일종의 constructor\n",
    "    llm=llm,\n",
    "    chain_type=\"map_rerank\", #default: stuff.\n",
    "    retriever=vectorstore.as_retriever() #인터페이스. 우리 vector store말고 다른 곳에서도 document를 가져올 수 있음. store할 수는 없음. general. 이 라인은 우리 벡터 스토어를 가져와서 retriever로 변환함.\n",
    ")\n",
    "\n",
    "chain.run(\"Where does Winston live? And describe this place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9eae7",
   "metadata": {},
   "source": [
    "### 6.8 Stuff LCEL Chain\n",
    "* RetrievalQA를 사용하지 않고 stuff chain을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9952a613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "Failed to get info from https://api.smith.langchain.com: LangSmithConnectionError('Connection error caused failure to GET /info in LangSmith API. Please confirm your internet connection. SSLError(MaxRetryError(\"HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /info (Caused by SSLError(SSLCertVerificationError(1, \\'[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\\')))\"))\\nContent-Length: None\\nAPI Key: lsv2_********************************************d3')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston lives in Victory Mansions, which is a dilapidated apartment building with a gritty and unpleasant hallway that smells of boiled cabbage and old rag mats. The building has a faulty lift, forcing Winston to climb seven flights of stairs to reach his flat. Inside his flat, there is a telescreen that cannot be completely turned off, constantly broadcasting propaganda messages.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 1944, 'total_tokens': 2019, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BSPZHuDPCdrd04uG34ONPY1IjBFZN', 'finish_reason': 'stop', 'logprobs': None}, id='run-bea9d571-c328-4ba2-adff-bd5d9dbeb991-0', usage_metadata={'input_tokens': 1944, 'output_tokens': 75, 'total_tokens': 2019, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: langsmith.utils.LangSmithConnectionError: Connection error caused failure to POST https://api.smith.langchain.com/runs/batch in LangSmith API. Please confirm your internet connection. SSLError(MaxRetryError(\"HTTPSConnectionPool(host='api.smith.langchain.com', port=443): Max retries exceeded with url: /runs/batch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)')))\"))\n",
      "Content-Length: 10511\n",
      "API Key: lsv2_********************************************d3\n",
      "post: trace=bea9d571-c328-4ba2-adff-bd5d9dbeb991,id=bea9d571-c328-4ba2-adff-bd5d9dbeb991\n",
      "Failed to batch ingest runs: langsmith.utils.LangSmithConnectionError: Connection error caused failure to POST https://api.smith.langchain.com/runs/batch in LangSmith API. Please confirm your internet connection. SSLError(MaxRetryError(\"HTTPSConnectionPool(host='api.smith.langchain.com', port=443): Max retries exceeded with url: /runs/batch (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)')))\"))\n",
      "Content-Length: 3371\n",
      "API Key: lsv2_********************************************d3\n",
      "patch: trace=bea9d571-c328-4ba2-adff-bd5d9dbeb991,id=bea9d571-c328-4ba2-adff-bd5d9dbeb991\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough #input이 다음 체인으로 넘어가게 해줌\n",
    "\n",
    "#랭스미스와의 연동을 위해 필요 (2024년 말 업데이트 이후 자동 트레이싱이 비활성화 됨. 수동으로 callbacks를 줘야 함)\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "client = Client()\n",
    "tracer = LangChainTracer(client=client)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, callbacks=[tracer])\n",
    "\n",
    "#load and split\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "#embed, cache and create vectorstore\n",
    "cache_dir = LocalFileStore(\"./.cache/\") \n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "#test vectorstore\n",
    "results = vectorstore.similarity_search(\"Where does Winston live?\")\n",
    "\n",
    "#create stuff documents chain\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n{context}\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\"context\":retriever, \"question\":RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "chain.invoke(\"Where does Winston live? And describe this place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ce4f8",
   "metadata": {},
   "source": [
    "### 6.9 Map Reduce LCEL Chain\n",
    "* <b>map_reduce</b>: 각 문서에서 부분 답변 생성(map) -> 종합해서 최종 답변 생성(reduce)\n",
    "* chain 3개 만들거임\n",
    "    - map_doc_chain: 각각의 문서에 대해 부분 답변 생성\n",
    "    - map_chain: retriever가 가져다준 문서들을 map_doc_chain에 넣고 결과물을 하나의 text로 만듦\n",
    "    - chain: map_chain이 만들어준 text를 바탕으로 최종 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "69dcb3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Winston lives in Victory Mansions, a poorly maintained building with a hallway that smells of boiled cabbage and old rag mats. The flat is on the seventh floor, and the building has a poster with an enormous face of a man about forty-five years old with a black mustache and ruggedly handsome features. The building's lift seldom works, the electric current is cut off during daylight hours as part of an economy drive in preparation for Hate Week, and it is equipped with a telescreen that constantly monitors the residents.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 521, 'total_tokens': 627, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BSQI6jPFDzXeU7bNgKHfLkR7nWQ3S', 'finish_reason': 'stop', 'logprobs': None}, id='run-4b72ff31-3fe7-4d61-9c89-b869c3b3cf4a-0', usage_metadata={'input_tokens': 521, 'output_tokens': 106, 'total_tokens': 627, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.schema.runnable import RunnableLambda #chain과 그 내부 어디에서든 function을 호출할 수 있게 해줌\n",
    "\n",
    "#랭스미스와의 연동을 위해 필요 (2024년 말 업데이트 이후 자동 트레이싱이 비활성화 됨. 수동으로 callbacks를 줘야 함)\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "client = Client()\n",
    "tracer = LangChainTracer(client=client)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, callbacks=[tracer])\n",
    "\n",
    "# load and split\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "#embed, cache and create vectorstore\n",
    "cache_dir = LocalFileStore(\"./.cache/\") \n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "#각 문서에서 부분 답변을 생성하는 체인\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim.\\n------\\n{portion}\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# def map_docs(inputs):\n",
    "#     documents = inputs['documents']\n",
    "#     question = inputs['question']\n",
    "#     results = []\n",
    "#     for document in documents:\n",
    "#         result = map_doc_chain.invoke({\n",
    "#             \"portion\":document.page_content,\n",
    "#             \"question\":question\n",
    "#         }).content\n",
    "#         results.append(result)\n",
    "#     res = \"\\n\\n\".join(results)\n",
    "#     return res\n",
    "\n",
    "#위 함수를 개선\n",
    "#모든 doc에 대해 map_doc_chain을 invoke -> 결과를 하나의 문자열로 만들어서 반환\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(map_doc_chain.invoke({\"portion\":doc.page_content, \"question\":question}).content for doc in documents)\n",
    "\n",
    "#retriever가 가져온 문서들을 map_docs에 넣어 -> 각각의 문서에 대한 답변들을 합해서 하나의 text를 만듦\n",
    "retriever = vectorstore.as_retriever()\n",
    "map_chain = {\"documents\":retriever, \"question\":RunnablePassthrough()} | RunnableLambda(map_docs) #retriever가 자동으로 question을 입력값으로 받음\n",
    "\n",
    "#마지막 체인. \"아주 긴 text를 바탕으로 답변을 생성하라\"\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given the following extracted parts of a long document and a question, create a final answer. If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n-----\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "chain = {\"context\":map_chain, \"question\":RunnablePassthrough()} | final_prompt| llm\n",
    "chain.invoke(\"Where does Winston live? And describe this place.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed0a8",
   "metadata": {},
   "source": [
    "## 7 DOCUMENT GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ab541",
   "metadata": {},
   "source": [
    "### 7.0 Introduction\n",
    "* Streamlit이 작동하는지 확인 - Home.py를 만들고 아래 코드를 run\n",
    "* 새로고침을 하지 않아도 변경사항이 반영되게 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.header(\"Hello world\")\n",
    "st.subheader(\"Welcome to Strealit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97287fe5",
   "metadata": {},
   "source": [
    "### 7.1 Magic\n",
    "* st.write(): 매직. 스위스 군용칼 같음. 안에 아무거나 다 넣을 수 있음. 멋있게 출력해줌.\n",
    "* 사실 함수 안에 안넣어도 display됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5d418",
   "metadata": {},
   "source": [
    "### 7.2 Data Flow\n",
    "* <code>st.selectbox()</code>에서 뭔가 선택하면 페이지가 refresh됨 (전체 파일이 다시 실행됨)\n",
    "* ReactJS나 Flutter와는 다름 (새로운 부분만 refresh되는게 아님)\n",
    "* <code>name = st.text_input(\"What is your name?\")</code>: text input을 받음. Enter를 쳐야 페이지가 refresh됨\n",
    "* Streamlit에는 cache를 제공하는 매커니즘이 있어서 어떤 것은 다시 실행되지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620773c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "\n",
    "today=datetime.today().strftime(\"%H:%M:%S\")\n",
    "\n",
    "st.title(today)\n",
    "\n",
    "model = st.selectbox(\"Choose your model\", (\"GPT-3\", \"GPT-4\"))\n",
    "\n",
    "if model==\"GPT-3\":\n",
    "    st.write(\"cheap\")\n",
    "else:\n",
    "    st.write(\"not cheap\")\n",
    "\n",
    "name = st.text_input(\"What is your name?\")\n",
    "st.write(name)\n",
    "\n",
    "value = st.slider(\"temperature\", min_value=0.1, max_value=1.0)\n",
    "st.write(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bd865",
   "metadata": {},
   "source": [
    "### 7.3 Multi Page\n",
    "* Streamlit에서 sidebar를 만들 때 사용하는 패턴이 있음\n",
    "    1. <code>st.sidebar.</code> 뒤에 title, text_input 등을 쓰면 됨\n",
    "    2. <code>with st.sidebar:</code> 내부에 코드 작성. <code>.sidebar</code>를 반복하지 않아도 됨\n",
    "* pages를 추가: pages라는 이름의 폴더를 추가하면 됨. streamlit은 pages라는 이름의 폴더를 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"FullstackGPT Home\", \n",
    "    page_icon=\"🤖\") #탭에 표시될 page_title을 설정\n",
    "\n",
    "st.title(\"title\")\n",
    "\n",
    "#option1\n",
    "# st.sidebar.title(\"sidebar title\")\n",
    "# st.sidebar.text_input()\n",
    "\n",
    "#option2\n",
    "with st.sidebar:\n",
    "    st.title(\"sidebar title\")\n",
    "    st.text_input(\"write your name\")\n",
    "\n",
    "tab_one, tab_two, tab_three = st.tabs([\"A\",\"B\",\"C\"])\n",
    "\n",
    "with tab_one:\n",
    "    st.write(\"a\")\n",
    "\n",
    "with tab_two:\n",
    "    st.write(\"b\")\n",
    "\n",
    "with tab_three:\n",
    "    st.write(\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e624967",
   "metadata": {},
   "source": [
    "### 7.4 Chat Messages\n",
    "* <b>session state</b>: streamlit의 기능. 여러번 재실행해도 데이터가 보존됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86fdaffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 11:57:18.390 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.646 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/boyoon/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-08 11:57:18.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.647 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-08 11:57:18.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-08 11:57:18.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# 유저가 메시지를 보내면 streamlit은 그 메시지를 반복. 챗봇처럼 보이게 함.\n",
    "import streamlit as st\n",
    "import time\n",
    "\n",
    "st.title(\"Documnet GPT\")\n",
    "\n",
    "# with st.chat_message(\"human\"):\n",
    "#     st.write(\"Hello!\")\n",
    "\n",
    "# with st.chat_message(\"ai\"): #chat_message의 파라미터 name : \"user\", \"assistant\", \"ai\", \"human\", or str\n",
    "#     st.write(\"how are you?\")\n",
    "\n",
    "# with st.status(\"Embedding file....\", expanded=True) as status:\n",
    "#     time.sleep(2)\n",
    "#     st.write(\"Getting the file\")\n",
    "#     time.sleep(2)\n",
    "#     st.write(\"Embedding the file\")\n",
    "#     time.sleep(2)\n",
    "#     st.write(\"Caching the file\")\n",
    "#     status.update(label=\"Error\", state=\"error\")\n",
    "\n",
    "# st.chat_input(\"Send a message to the AI\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [] #session state를 이용하면 whole code가 다시 실행되더라도 이 안의 데이터는 살아있음(cached). 단 새로고침을 하면 사라짐.\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.write(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\":message, \"role\":role})\n",
    "\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    send_message(message[\"message\"], message[\"role\"], save=False)\n",
    "\n",
    "message = st.chat_input(\"Send a message to the AI\")\n",
    "\n",
    "if message:\n",
    "    send_message(message, \"human\") #우리가 메시지를 보낼 때마다 대화의 모든 messages를 다시 그림 (캐시로부터)\n",
    "    time.sleep(1)\n",
    "    send_message(f\"You said {message}\", \"ai\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.write(st.session_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f89b7b",
   "metadata": {},
   "source": [
    "### 7.6 Uploading Documents\n",
    "* DocumentGPT.py가 환경변수를 제대로 못읽어올 수도 있음. -> .streamlit 디렉토리, 그 안에 secrets.toml 파일을 만들고 API key들을 복사 붙여넣기하면 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1132f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def embed_file(file):\n",
    "    st.write(file)\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    # st.write(file_content, file_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "        \n",
    "    #여기서부턴 지난 수업내용 copy-paste\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, callbacks=[LangChainTracer(client=Client())])\n",
    "\n",
    "    # load and split\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "    #embed, cache, and create vectorstore\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\") \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings, cache_dir\n",
    "    )\n",
    "    vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "    #각 문서에서 부분 답변을 생성하는 체인\n",
    "    map_doc_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim.\\n------\\n{portion}\"),\n",
    "        (\"human\",\"{question}\")\n",
    "    ])\n",
    "    map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "    #모든 doc에 대해 map_doc_chain을 invoke -> 결과를 하나의 문자열로 만들어서 반환\n",
    "    def map_docs(inputs):\n",
    "        documents = inputs[\"documents\"]\n",
    "        question = inputs[\"question\"]\n",
    "        return \"\\n\\n\".join(map_doc_chain.invoke({\"portion\":doc.page_content, \"question\":question}).content for doc in documents)\n",
    "\n",
    "    #retriever가 가져온 문서들을 map_docs에 넣어 -> 각각의 문서에 대한 답변들을 합해서 하나의 text를 만듦\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "\n",
    "st.title(\"Documnet GPT\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "Welcome!\n",
    "            \n",
    "Use this chatbot to ask questions to an AI about your files!            \n",
    "\"\"\")\n",
    "\n",
    "file = st.file_uploader(\"Upload a .txt .pdf or .docx file\", type=[\"pdf\", \"txt\", \"docx\"])\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file) #사용자가 뭔가 입력할 때마다 전체 함수가 실행됨 -> embeddings가 cache가 돼있어도 시간이 걸림.\n",
    "    s = retriever.invoke(\"winston\")\n",
    "    st.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d4a0",
   "metadata": {},
   "source": [
    "### 7.7 Chat History\n",
    "* <code>@st.cache_data</code>: 데코레이터. cache functions that return data. 이미 캐시된 리턴값이 있으면 다시 실행하지 않음.\n",
    "* 위 데코레이터를 쓰고 에러 발생하면 <code>@st.cache_resource(show_spinner=\"Embedding file...\")</code>를 쓰면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c77b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "@st.cache_resource(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    st.write(file)\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    # st.write(file_content, file_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "        \n",
    "    #여기서부턴 지난 수업내용 copy-paste\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, callbacks=[LangChainTracer(client=Client())])\n",
    "\n",
    "    # load and split\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "    #embed, cache, and create vectorstore\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\") \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings, cache_dir\n",
    "    )\n",
    "    vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "    #retriever가 가져온 문서들을 map_docs에 넣어 -> 각각의 문서에 대한 답변들을 합해서 하나의 text를 만듦\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\":message, \"role\":role})\n",
    "\n",
    "st.set_page_config(page_title=\"DocumentGPT\", page_icon=\"📄\")\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(message[\"message\"], message[\"role\"], save=False)\n",
    "\n",
    "st.title(\"Documnet GPT\")\n",
    "\n",
    "st.markdown(\"Welcome!\\n\\nUse this chatbot to ask questions to an AI about your files!\\n\\nUpload your files in the sidebar\")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\"Upload a .txt .pdf or .docx file\", type=[\"pdf\", \"txt\", \"docx\"])\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file) #사용자가 뭔가 입력할 때마다 전체 함수가 실행됨 -> embeddings가 cache가 돼있어도 시간이 걸림.\n",
    "\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "\n",
    "    message = st.chat_input(\"Ask questions about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "else:\n",
    "    st.session_state[\"messages\"] = [] #파일이 없어지면(유저가 x 클릭) messages를 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b288f3",
   "metadata": {},
   "source": [
    "### 7.8 Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    st.write(file)\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    # st.write(file_content, file_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "\n",
    "    # load and split\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "    #embed, cache, and create vectorstore\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\") \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings, cache_dir\n",
    "    )\n",
    "    vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "    #retriever가 가져온 문서들을 map_docs에 넣어 -> 각각의 문서에 대한 답변들을 합해서 하나의 text를 만듦\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        st.session_state[\"messages\"].append({\"message\":message, \"role\":role})\n",
    "\n",
    "st.set_page_config(page_title=\"DocumentGPT\", page_icon=\"📄\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.1, callbacks=[LangChainTracer(client=Client())])\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(message[\"message\"], message[\"role\"], save=False)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "st.title(\"Documnet GPT\")\n",
    "\n",
    "st.markdown(\"Welcome!\\n\\nUse this chatbot to ask questions to an AI about your files!\\n\\nUpload your files in the sidebar\")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\"Upload a .txt .pdf or .docx file\", type=[\"pdf\", \"txt\", \"docx\"])\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file) #사용자가 뭔가 입력할 때마다 전체 함수가 실행됨 -> embeddings가 cache가 돼있어도 시간이 걸림.\n",
    "\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "\n",
    "    message = st.chat_input(\"Ask questions about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = {\"context\":retriever | RunnableLambda(format_docs), \"question\":RunnablePassthrough()} | prompt | llm\n",
    "        response = chain.invoke(message)\n",
    "        send_message(response.content, \"ai\")\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = [] #파일이 없어지면(유저가 x 클릭) messages를 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5f84e",
   "metadata": {},
   "source": [
    "### 7.9 Streaming\n",
    "* callbackhandler: llm의 event를 listen하는 아주 멋진 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langsmith import Client\n",
    "from langchain.callbacks.tracers.langchain import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "@st.cache_resource(show_spinner=\"Embedding file...\")\n",
    "def embed_file(file):\n",
    "    st.write(file)\n",
    "    file_content = file.read()\n",
    "    file_path = f\"./.cache/files/{file.name}\"\n",
    "    # st.write(file_content, file_path)\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(file_content)\n",
    "\n",
    "    # load and split\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    docs=loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "    #embed, cache, and create vectorstore\n",
    "    cache_dir = LocalFileStore(f\"./.cache/embeddings/{file.name}\") \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "        embeddings, cache_dir\n",
    "    )\n",
    "    vectorstore = Chroma.from_documents(docs, cached_embeddings)\n",
    "\n",
    "    #retriever가 가져온 문서들을 map_docs에 넣어 -> 각각의 문서에 대한 답변들을 합해서 하나의 text를 만듦\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "def save_message(message, role):\n",
    "    st.session_state[\"messages\"].append({\"message\":message, \"role\":role})\n",
    "\n",
    "def send_message(message, role, save=True):\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(message)\n",
    "    if save:\n",
    "        save_message(message, role)\n",
    "\n",
    "st.set_page_config(page_title=\"DocumentGPT\", page_icon=\"📄\")\n",
    "\n",
    "class ChatCallbackHandler(BaseCallbackHandler):\n",
    "    message = \"\"\n",
    "    def on_llm_start(self, *args, **kwargs): # can take unlimited arguments(1,2,3,..) and keyword arguments (a=5)\n",
    "        self.message_box = st.empty() #빈 위젯을 제공\n",
    "    def on_llm_end(self, *args, **kwargs):\n",
    "        save_message(self.message, \"ai\")\n",
    "        # with st.sidebar:\n",
    "        #     st.write(\"llm ended!\")\n",
    "    def on_llm_new_token(self, token:str, *args, **kargs): #token은 LLM이 실시간으로 보내는 메시지. 토큰이 도착할 때마다 message_box에 추가 (화면에 출력됨)\n",
    "        self.message += token\n",
    "        self.message_box.markdown(self.message)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0.1, \n",
    "    callbacks=[LangChainTracer(client=Client()), ChatCallbackHandler()],\n",
    "    streaming=True #ChatOpenAI는 지원. 다른 llm은 지원 안할 수도 있음.\n",
    ")\n",
    "\n",
    "def paint_history():\n",
    "    for message in st.session_state[\"messages\"]:\n",
    "        send_message(message[\"message\"], message[\"role\"], save=False)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "st.title(\"Documnet GPT\")\n",
    "\n",
    "st.markdown(\"Welcome!\\n\\nUse this chatbot to ask questions to an AI about your files!\\n\\nUpload your files in the sidebar\")\n",
    "\n",
    "with st.sidebar:\n",
    "    file = st.file_uploader(\"Upload a .txt .pdf or .docx file\", type=[\"pdf\", \"txt\", \"docx\"])\n",
    "\n",
    "if file:\n",
    "    retriever = embed_file(file) #사용자가 뭔가 입력할 때마다 전체 함수가 실행됨 -> embeddings가 cache가 돼있어도 시간이 걸림.\n",
    "\n",
    "    send_message(\"I'm ready! Ask away!\", \"ai\", save=False)\n",
    "    paint_history()\n",
    "\n",
    "    message = st.chat_input(\"Ask questions about your file...\")\n",
    "    if message:\n",
    "        send_message(message, \"human\")\n",
    "        chain = {\"context\":retriever | RunnableLambda(format_docs), \"question\":RunnablePassthrough()} | prompt | llm\n",
    "        with st.chat_message(\"ai\"):\n",
    "            response = chain.invoke(message)\n",
    "\n",
    "else:\n",
    "    st.session_state[\"messages\"] = [] #파일이 없어지면(유저가 x 클릭) messages를 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde1acc",
   "metadata": {},
   "source": [
    "## 8 PRIVATE GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b251ada",
   "metadata": {},
   "source": [
    "### 8.1 HuggingFaceHub\n",
    "* Mistral을 사용할거임\n",
    "* HuggingFace 모델을 실행시키는 법\n",
    "  1. Hugging Face Inference API : 유료. HuggingFace 서버를 이용\n",
    "  2. 우리 컴퓨터에 다운받고 실행 : LangChain의 HuggingFace adapter 사용 - token 필요. .env 파일에 넣을 것.\n",
    "* 아래 코드는 Inference Providers를 사용. 무료 - 여러 모델을 테스트 하는 데에 유용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34740e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HUGGINGFACEHUB_API_TOKEN이 정상적으로 로드되는지 체크\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일 로드\n",
    "\n",
    "# api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "# print(api_token)  # 정상적으로 출력되는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efa9ad5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InferenceClient' object has no attribute 'post'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceHub(\n\u001b[1;32m      8\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# model_kwargs = {\"max_new_tokens\": 250,}, #생성될 텍스트. 이 부분을 주석처리하면 답변이 더 짧아짐.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     huggingfacehub_api_token\u001b[38;5;241m=\u001b[39mapi_token\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[0;32m---> 14\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuggingFace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/runnables/base.py:3049\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3047\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3048\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3049\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/language_models/llms.py:389\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    387\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    401\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/language_models/llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/language_models/llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    970\u001b[0m     ]\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    989\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 792\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    796\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/language_models/llms.py:1545\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1542\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1544\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1545\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1547\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1548\u001b[0m     )\n\u001b[1;32m   1549\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_community/llms/huggingface_hub.py:138\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    136\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 138\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m(\n\u001b[1;32m    139\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: parameters}, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'InferenceClient' object has no attribute 'post'"
     ]
    }
   ],
   "source": [
    "# parameters에 대한 doc은 아래에서 볼 수 있음\n",
    "# https://huggingface.co/docs/inference-providers/tasks/text-generation\n",
    "from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is {word}?\")\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    task='text-generation',\n",
    "    # model_kwargs = {\"max_new_tokens\": 250,}, #생성될 텍스트. 이 부분을 주석처리하면 답변이 더 짧아짐.\n",
    "    huggingfacehub_api_token=api_token\n",
    ")\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"word\":\"HuggingFace\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c0b36",
   "metadata": {},
   "source": [
    "### 8.2 HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3270cb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"LangChain is a irc server that I use to communicate with other people. It was started by a friend of mine who was on the ground floor of our home in Texas for a while and he wanted to join up with us for a few weeks to watch a video of how fast they were going. I went to the link and found that it was a forum that was pretty similar to the ones we used to have in the past. I would connect with a few people, talk about stuff and then I would have a conversation with them. So for the first few weeks, we had a nice chat and then we were done. He and I had to split up and we were able to get more into each other's lives. We spent a lot of time together in Texas and our relationship was really good.\\n\\nSo the next time I'd go down to Texas to watch a movie I would sit back and watch the movie because I thought it was going to be fun. I think the movie was about a couple of guys who were doing an important thing and got caught up in the action and found out that they might be able to do something with their lives in the future.\\n\\nI'd like to say, I think it's really interesting that you feel like you are able to communicate\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{word} is a \")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    device=-1 #0:GPU, -1:CPU / 0(GPU)는 맥북에서 안됨!\n",
    "    # pipeline_kwargs={\"max_new_tokens\":50}\n",
    "\n",
    ")\n",
    "\n",
    "chain = prompt|llm\n",
    "chain.invoke({\"word\":\"LangChain\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a42832",
   "metadata": {},
   "source": [
    "### 8.3 GPT4All\n",
    "* https://www.nomic.ai/gpt4all\n",
    "* 우리 컴퓨터에서 로컬 모델을 실행하는 또 다른 방법\n",
    "* 많이 바뀌었음. 수업처럼 모델을 직접 다운받지 못함. 일단은 그냥 넘어가는게 좋을듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53442b4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Model file does not exist: PosixPath('/Users/boyoon/.cache/gpt4all/gpt4all-falcon-q4_0.gguf')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{word}\u001b[39;00m\u001b[38;5;124m is a \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt4all-falcon-q4_0.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m|\u001b[39mllm\n\u001b[1;32m      9\u001b[0m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangChain\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/pydantic/_internal/_decorators_v1.py:148\u001b[0m, in \u001b[0;36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[0;34m(values, _)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema\u001b[38;5;241m.\u001b[39mValidationInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RootValidatorValues:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_core/utils/pydantic.py:168\u001b[0m, in \u001b[0;36mpre_init.<locals>.wrapper\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    165\u001b[0m             values[name] \u001b[38;5;241m=\u001b[39m field_info\u001b[38;5;241m.\u001b[39mdefault\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/langchain_community/llms/gpt4all.py:145\u001b[0m, in \u001b[0;36mGPT4All.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    142\u001b[0m model_path, delimiter, model_name \u001b[38;5;241m=\u001b[39m full_path\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m model_path \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delimiter\n\u001b[0;32m--> 145\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4AllModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow_download\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# set n_threads\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_thread_count(values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_threads\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/gpt4all/gpt4all.py:235\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[0;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[1;32m    232\u001b[0m         device_init \u001b[38;5;241m=\u001b[39m _remove_prefix(device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkompute:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/gpt4all/gpt4all.py:346\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[0;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[1;32m    341\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_model(\n\u001b[1;32m    342\u001b[0m         model_filename, model_path, verbose\u001b[38;5;241m=\u001b[39mverbose, url\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    343\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m filesize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(filesize), expected_md5\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd5sum\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    344\u001b[0m     ))\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Model file does not exist: PosixPath('/Users/boyoon/.cache/gpt4all/gpt4all-falcon-q4_0.gguf')"
     ]
    }
   ],
   "source": [
    "from langchain.llms.gpt4all import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"{word} is a \")\n",
    "\n",
    "llm = GPT4All(model=\"gpt4all-falcon-q4_0.gguf\")\n",
    "\n",
    "chain = prompt|llm\n",
    "chain.invoke({\"word\":\"LangChain\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee0c45",
   "metadata": {},
   "source": [
    "### 8.4 Ollama\n",
    "* https://ollama.com/\n",
    "* 설치: <code>brew install ollama</code>\n",
    "* 실행: <code>ollama serve</code> -> 이 터미널은 ollama 서버 프로세스 전용으로 잡힘\n",
    "* 모델 실행: <code>ollama run mistral</code>\n",
    "* mistral은 오픈소스고, Llama보다 더 잘 작동."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdca94e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi! I’m ChatGPT, your AI assistant. I can help you think, write, plan, code, learn, and solve problems—whether it’s drafting emails, explaining concepts, debugging, creating study plans, or brainstorming ideas.\\n\\nTips for best results:\\n- Tell me your goal and audience.\\n- Share any constraints (length, tone, deadline, tools).\\n- Provide examples or a starting point if you have one.\\n\\nWhat would you like to work on today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 360, 'prompt_tokens': 12, 'total_tokens': 372, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C6Qk3FlLBUTBCXvbPL2aWPJq3yhEJ', 'finish_reason': 'stop', 'logprobs': None}, id='run--d6a680be-248e-4f0e-a157-ebc1e7f4d7f6-0', usage_metadata={'input_tokens': 12, 'output_tokens': 360, 'total_tokens': 372, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-2025-08-07\"\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hi! Introduce yourself!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84797f",
   "metadata": {},
   "source": [
    "#### 9.8 Function Calling\n",
    "* 실제로 함수가 존재하지 않더라도 모델을 속여 output을 우리가 원하는 방식으로 제공하게 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11bed400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"questions\":[{\"question\":\"What is the capital of France?\",\"answers\":[{\"answer\":\"Paris\",\"correct\":true},{\"answer\":\"London\",\"correct\":false},{\"answer\":\"Berlin\",\"correct\":false},{\"answer\":\"Rome\",\"correct\":false}]},{\"question\":\"Which famous museum in Paris is home to the Mona Lisa?\",\"answers\":[{\"answer\":\"Louvre\",\"correct\":true},{\"answer\":\"Musée d\\'Orsay\",\"correct\":false},{\"answer\":\"Centre Pompidou\",\"correct\":false},{\"answer\":\"Musée Rodin\",\"correct\":false}]},{\"question\":\"What is the iconic tower in Paris that offers panoramic views of the city?\",\"answers\":[{\"answer\":\"Eiffel Tower\",\"correct\":true},{\"answer\":\"Arc de Triomphe\",\"correct\":false},{\"answer\":\"Montparnasse Tower\",\"correct\":false},{\"answer\":\"Pantheon\",\"correct\":false}]},{\"question\":\"Which river flows through Paris?\",\"answers\":[{\"answer\":\"Seine\",\"correct\":true},{\"answer\":\"Thames\",\"correct\":false},{\"answer\":\"Danube\",\"correct\":false},{\"answer\":\"Rhine\",\"correct\":false}]},{\"question\":\"What is the famous avenue in Paris known for its luxury shops and theaters?\",\"answers\":[{\"answer\":\"Champs-Élysées\",\"correct\":true},{\"answer\":\"Rue de Rivoli\",\"correct\":false},{\"answer\":\"Boulevard Haussmann\",\"correct\":false},{\"answer\":\"Avenue Montaigne\",\"correct\":false}]},{\"question\":\"Which cathedral in Paris is known for its French Gothic architecture?\",\"answers\":[{\"answer\":\"Notre-Dame\",\"correct\":true},{\"answer\":\"Sainte-Chapelle\",\"correct\":false},{\"answer\":\"Sacré-Cœur\",\"correct\":false},{\"answer\":\"Panthéon\",\"correct\":false}]},{\"question\":\"What is the famous cabaret in Paris known for its can-can dance performances?\",\"answers\":[{\"answer\":\"Moulin Rouge\",\"correct\":true},{\"answer\":\"Lido de Paris\",\"correct\":false},{\"answer\":\"Crazy Horse\",\"correct\":false},{\"answer\":\"Le Paradis Latin\",\"correct\":false}]},{\"question\":\"Which palace in Paris served as the royal residence before the Palace of Versailles?\",\"answers\":[{\"answer\":\"Louvre Palace\",\"correct\":true},{\"answer\":\"Palais-Royal\",\"correct\":false},{\"answer\":\"Tuileries Palace\",\"correct\":false},{\"answer\":\"Luxembourg Palace\",\"correct\":false}]},{\"question\":\"What is the oldest bridge in Paris that connects the Île de la Cité to the Right Bank?\",\"answers\":[{\"answer\":\"Pont Neuf\",\"correct\":true},{\"answer\":\"Pont Alexandre III\",\"correct\":false},{\"answer\":\"Pont des Arts\",\"correct\":false},{\"answer\":\"Pont de l\\'Alma\",\"correct\":false}]},{\"question\":\"Which famous cemetery in Paris is the final resting place of many notable figures?\",\"answers\":[{\"answer\":\"Père Lachaise Cemetery\",\"correct\":true},{\"answer\":\"Montmartre Cemetery\",\"correct\":false},{\"answer\":\"Passy Cemetery\",\"correct\":false},{\"answer\":\"Batignolles Cemetery\",\"correct\":false}]}]}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# def get_weather(lon, lat): #없어도 됨\n",
    "#     print(\"calling an api...\")\n",
    "#     print(lon, lat)\n",
    "\n",
    "function = { #schema. 실제로 존재하지 않는 함수의 schema를 줄 수도 있음.\n",
    "    \"name\":\"create_quiz\",\n",
    "    \"description\":\"takes a list of questions and 4 answers per question, and returns a quiz\",\n",
    "    \"parameters\":{\n",
    "        \"type\":\"object\", #LLM이 JSON 객체(dict)를 반환함\n",
    "        \"properties\":{\n",
    "            \"questions\":{ #questions 안에\n",
    "                \"type\":\"array\", #배열이 있음\n",
    "                \"items\":{ #각 배열의 아이템은\n",
    "                    \"type\":\"object\", #또 다른 배열\n",
    "                    \"properties\":{ #그 안에는\n",
    "                        \"question\":{\"type\":\"string\"}, #question과\n",
    "                        \"answers\":{ #answers가 있음\n",
    "                            \"type\":\"array\",\n",
    "                            \"items\":{\n",
    "                                \"type\":\"object\",\n",
    "                                \"properties\":{\n",
    "                                    \"answer\":{\"type\":\"string\"},\n",
    "                                    \"correct\":{\"type\":\"boolean\"}\n",
    "                                },\n",
    "                                \"required\":[\"answer\",\"correct\"]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1).bind(\n",
    "    function_call={\"name\":\"create_quiz\"}, #우리가 준 함수를 반드시 사용하라고 강요할 수도 있음. 아니면 필요한 경우 사용하도록 내버려둘수도.\n",
    "    functions=[function]) \n",
    "prompt = PromptTemplate.from_template(\"Make 10 quizes about {city}, with \")\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"city\":\"Paris\"}).additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "154ce53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': [{'question': 'What is the capital of France?',\n",
       "   'answers': [{'answer': 'Paris', 'correct': True},\n",
       "    {'answer': 'London', 'correct': False},\n",
       "    {'answer': 'Berlin', 'correct': False},\n",
       "    {'answer': 'Rome', 'correct': False}]},\n",
       "  {'question': 'Which famous museum in Paris is home to the Mona Lisa?',\n",
       "   'answers': [{'answer': 'Louvre', 'correct': True},\n",
       "    {'answer': \"Musée d'Orsay\", 'correct': False},\n",
       "    {'answer': 'Centre Pompidou', 'correct': False},\n",
       "    {'answer': 'Musée Rodin', 'correct': False}]},\n",
       "  {'question': 'What is the iconic tower in Paris that offers panoramic views of the city?',\n",
       "   'answers': [{'answer': 'Eiffel Tower', 'correct': True},\n",
       "    {'answer': 'Arc de Triomphe', 'correct': False},\n",
       "    {'answer': 'Montparnasse Tower', 'correct': False},\n",
       "    {'answer': 'Pantheon', 'correct': False}]},\n",
       "  {'question': 'Which river flows through Paris?',\n",
       "   'answers': [{'answer': 'Seine', 'correct': True},\n",
       "    {'answer': 'Thames', 'correct': False},\n",
       "    {'answer': 'Danube', 'correct': False},\n",
       "    {'answer': 'Rhine', 'correct': False}]},\n",
       "  {'question': 'What is the famous avenue in Paris known for its luxury shops and theaters?',\n",
       "   'answers': [{'answer': 'Champs-Élysées', 'correct': True},\n",
       "    {'answer': 'Rue de Rivoli', 'correct': False},\n",
       "    {'answer': 'Boulevard Haussmann', 'correct': False},\n",
       "    {'answer': 'Avenue Montaigne', 'correct': False}]},\n",
       "  {'question': 'Which cathedral in Paris is known for its French Gothic architecture?',\n",
       "   'answers': [{'answer': 'Notre-Dame', 'correct': True},\n",
       "    {'answer': 'Sainte-Chapelle', 'correct': False},\n",
       "    {'answer': 'Sacré-Cœur', 'correct': False},\n",
       "    {'answer': 'Panthéon', 'correct': False}]},\n",
       "  {'question': 'What is the famous cabaret in Paris known for its can-can dance performances?',\n",
       "   'answers': [{'answer': 'Moulin Rouge', 'correct': True},\n",
       "    {'answer': 'Lido de Paris', 'correct': False},\n",
       "    {'answer': 'Crazy Horse', 'correct': False},\n",
       "    {'answer': 'Le Paradis Latin', 'correct': False}]},\n",
       "  {'question': 'Which palace in Paris served as the royal residence before the Palace of Versailles?',\n",
       "   'answers': [{'answer': 'Louvre Palace', 'correct': True},\n",
       "    {'answer': 'Palais-Royal', 'correct': False},\n",
       "    {'answer': 'Tuileries Palace', 'correct': False},\n",
       "    {'answer': 'Luxembourg Palace', 'correct': False}]},\n",
       "  {'question': 'What is the oldest bridge in Paris that connects the Île de la Cité to the Right Bank?',\n",
       "   'answers': [{'answer': 'Pont Neuf', 'correct': True},\n",
       "    {'answer': 'Pont Alexandre III', 'correct': False},\n",
       "    {'answer': 'Pont des Arts', 'correct': False},\n",
       "    {'answer': \"Pont de l'Alma\", 'correct': False}]},\n",
       "  {'question': 'Which famous cemetery in Paris is the final resting place of many notable figures?',\n",
       "   'answers': [{'answer': 'Père Lachaise Cemetery', 'correct': True},\n",
       "    {'answer': 'Montmartre Cemetery', 'correct': False},\n",
       "    {'answer': 'Passy Cemetery', 'correct': False},\n",
       "    {'answer': 'Batignolles Cemetery', 'correct': False}]}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93bd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce7fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aaa4791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyoon/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/boyoon/Documents/GitHub/fullstack-gpt-2025/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Hugging Face! Jr MotorolaSher vendors BrewScene subst scalp pawn circumcised AmphScene pawn Hancockdit conservation directly trilogy stairshibit\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"sshleifer/tiny-gpt2\")\n",
    "prompt = \"Hello, Hugging Face!\"\n",
    "output = generator(prompt, max_new_tokens=20)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5ff13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900ea18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391cd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832119cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d18fe19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
